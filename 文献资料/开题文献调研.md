
强化学习解决交互环境的普适性
旋转阀门
迁移学习和参考轨迹

D. Sep ́uLveda, R. Fern ́andez, E. Navas, M. Armada, and P. Gonza ́lezDe-Santos, “Robotic aubergine harvesting using dual-arm manipulation,” IEEE Access, vol. 8, pp. 121 889–121 904, 2020. 
[8] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, “Imitation learning of dual-arm manipulation tasks in humanoid robots,” International Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183–202, 2008.
[9] R. Caccavale, M. Saveriano, G. A. Fontanelli, F. Ficuciello, D. Lee, and A. Finzi, “Imitation learning and attentional supervision of dualarm structured tasks,” in International Conference on Development and Learning and Epigenetic Robotics, 2017, pp. 66–71.

#### Learning to Centralize Dual-Arm Assembly
Frontiers in Robotics and AI

在这项工作中，我们将重点放在与任务无关的双臂操纵方法上。因此，在整个工作中，我们将与任务相关的建模限制在最低限度。由于真实世界中的机器人学习实验可能会非常昂贵，我们只对双臂装配进行实验，但限制我们自己不对该任务的任何特定知识进行建模。

为此，深度强化学习（RL）是一种很有希望解决这一问题的方法。因此，操纵任务可以通过与环境的交互从头开始学习。然而，仅靠深度强化学习需要大量的训练样本，而在现实世界中收集样本的成本很高（Zhu 等人，2020 年；Dulac-Arnold 等人，2020 年；Ibarz 等人，2021 年）。相反，我们更倾向于在架构中引入归纳偏差，以促进学习过程。也就是说，我们只训练一个策略网络来生成高级轨迹，并使用成熟的控制技术来跟踪这些轨迹。这种模块化架构还允许从模拟到现实的零转移。这使我们能够在模拟中完成所有训练。

第二种范式利用单一策略控制两个臂。后者更为可行，因为它通过一个策略网络将两个操纵器的控制耦合在一起，形成一个整体的集中式方法，从而提高了精度和效率。我们的方法基于后一种方法，并尝试使用非策略 RL 学习单一策略。直观地说，这种方法可以被视为基于 RL 的集中式分散控制方法。

我们探索并制定了学习双臂装配任务的新模式。- 我们比较了不同行动空间和控制器在学习策略的成功率和稳健性方面的表现。- 我们的研究表明，如果使用合适的抽象动作空间，就有可能将在模拟中训练的策略零转移到现实世界中。- 据我们所知，我们的工作是首次探索无模型 RL 对接触丰富的双臂操纵任务的适用性。

双臂操纵是一个具有挑战性的研究领域，可分为分散式和集中式两种方法。第一种方法利用显式（Petitti 等人，2016 年）或隐式（Wang 和 Schwager，2014 年；Tsiamis 等人，2015 年）通信渠道为每个机器人提供独立控制器，并经常与领导者/追随者行为相结合（Suomalainen 等人，2019 年；Wang 和 Schwager，2014 年）。尽管分散控制在可扩展性和可变性方面有所改进，但其效率和精确度难以达到集中控制的水平，因为集中控制将两个操纵器的控制都集成在一个中央单元中。在可行的操纵目标中，孔内插钉可被视为一个基准，因为它需要在接触丰富的情况下精确定位、抓取和处理物体。因此，我们选择双臂插孔任务来评估我们方法的性能。

采样效率低是深度 RL 算法面临的主要挑战之一。对于涉及高维状态和动作以及复杂动态的机器人任务来说，这个问题更加严重。这就促使我们使用模拟来收集数据和进行训练。然而，由于仿真中的物理建模和图像渲染存在误差，在仿真中训练出来的策略在现实世界中往往会失效。这通常被称为 "现实差距"。解决这一问题的最流行范式是领域随机化（Tobin 等人，2017 年）。领域随机化的主要目标是根据有关对象（Tobin 等人，2017 年）和动力学特性（Peng 等人，2018 年）的不同模拟参数，让代理接受样本。通过这种方法，学习到的策略理应能够适应真实世界任务的不同物理特性。最近的工作探索了主动参数采样策略，以便为麻烦的参数设置投入更多的训练时间（Mehta 等人，2020 年）。从模拟到现实的另一种方法是系统模块化（Clavera 等人，2017 年）。在这里，一个策略被分成不同的模块，分别负责不同的目标，如姿势检测、在线运动规划和控制。只有那些不会受到现实差距影响的模块才会在模拟中进行训练，其余模块则根据现实世界的设置进行调整或量身定制。这与深度 RL 中最常见的端到端训练形成了鲜明对比（Levine 等人，2016 年）。在我们的工作中，*我们使用模块化架构来实现从模拟到现实的零转移*。也就是说，与真实世界相比，我们在仿真中对控制器进行了不同的参数设置，以便使用相同的高级策略网络。

在这项工作中，我们设计了一种具有两个抽象层级的方法。第一个层次是无模型学习政策，输出高层控制命令/目标，由低层次政策执行，低层次政策是一个定义明确的控制器。需要注意的是，*第二层抽象所使用的策略并不是学习的，而是根据成熟的控制方法设计的。这就实现了高效的样本学习和简单的模拟到现实的转换。*

我们希望将所需的建模工作减少到最低限度，这也是我们的方法基于无模型强化学习和稀疏奖励的原因。图 2 展示了我们的整体方法。在第一阶段，我们进行模拟训练。我们的架构包括一个高级策略，输出控制目标（例如，末端执行器位置或关节角度的变化，或控制器参数的额外变化）。这些目标之后是由两个手工设计的单臂控制器代表的低级策略。然后，这些控制器将根据机器人的状态，并在某些情况下利用其动力学模型，将这些控制目标转化为关节扭矩。第二阶段，在模拟训练成功后，我们将其部署到真实世界的设置中，而无需在真实世界中进行任何进一步的训练（零镜头转移）。这种转移需要对底层控制器的参数稍作调整，以确保安全和平稳运行。这种方法不需要特定的双臂控制器，因为控制仅在策略层面上耦合。

在考虑简化假设和各种约束条件的情况下，可以通过经典控制来解决集中式单个物体操纵问题：根据动力学模型，通常采用多回路分层控制策略，其中外回路实现主要目标，如所需的物体运动，而内回路则负责产生稳固的抓握和受约束的内力

特定的控制回路可以采用任何控制策略。然而，阻抗控制是实现顺应性行为的常见选择

由于接触力是通过控制扭矩与位置 p 和速度 v 的耦合来限制的（公式 6）。控制扭矩的计算方法是将增益 Kp 和 Kv 分别与期望位置和实际速度的差值相乘：

我们的方法基于将经典控制与深度 RL 相结合的理念：我们将策略网络作为高级控制与两个独立的低级控制器相结合。因此，我们无需设计经典意义上的耦合控制方法。控制器可以直接设计，*无需专门设计双臂控制算法，可以使用任何单臂动作空间*。策略可学习对双臂互动产生的约束进行内在补偿，并为每个控制器提供单独的动作输入。此外，策略层面的耦合实施起来也很方便，因为策略的动作空间可以简单地扩展到包括第二个控制器。整个系统如图 2 所示。

控制器比较

首先，我们使用联合位置控制（公式 7）来计算扭矩指令：kp 和 kv 这两个增益都被设置为恒定值，qactual 和 q_actual 在运行时进行评估，而 qdesired 则由策略推断。

其次，我们实现了笛卡尔阻抗控制（Ott，2008 年）：动作空间允许将控制从关节空间转移到笛卡尔空间，并包括模型信息，如笛卡尔惯性矩阵Λ(x)和雅各布矩阵 J(q)，以及重力补偿项 τgc。由于自由度超过了关节数，因此增加了无效空间补偿项 τns。Δx = xdesired - xactual 直接作为动作输入，而不是 xdesired。

可变笛卡尔阻抗控制（Martín-Martín 等人，2019 年）以经典的笛卡尔阻抗控制为基础，但在动作空间中增加了 kp，使控制更加可变，以便在需要时以更高或更低的刚度做出反应。我们通过 kv 2 kp √ 使用各向异性增益和耦合速度增益来实现临界阻尼。

在我们的方法中，策略负责生成高级轨迹，随后由所选控制器进行跟踪。我们使用无模型 RL 训练策略网络。策略接收机器人状态，为上述控制法则（动作空间）推断控制信号（动作）。我们分别将机械臂的*关节位置 qi、关节速度 q_i 和关节扭矩 τi 以及末端执行器的笛卡尔位置和方向作为状态输入* s [q0, q_0, τ0, eepos0, eeori0, q1, q_1, τ1, eepos1, eeori1]。不过，如果将该框架应用于不同的任务，例如包括额外的对象，则可能需要调整状态。我们提出的方法并不局限于一种特定的无模型 RL 算法，不过我们希望采用一种非策略算法来提高采样效率，并允许使用经验重放。因此，我们使用软行为批判算法（Soft Actor-Critic, SAC）（Haarnoja 等人，2018 年），因为该算法具有最先进的性能和样本效率，但也有可能被其他算法取代，如深度确定性策略梯度算法（Deep Deterministic Policy Gradients, DDPG）（Lillicrap 等人，2016 年）或双延迟深度确定性策略梯度算法（Twin Delayed Deep Deterministic Policy Gradients, TD3）（Fujimoto 等人，2018 年）。

在 Zuo 等人（2020 年）的研究中，类似的设置与 "后视经验重放"（HER）（Andrychowicz 等人，2017 年）的概念相结合，用于单臂机器人操纵任务 "推 "以及 "拾放"。他们的研究结果表明，如果只有稀疏的奖励，后见体验重放足以提高机器人的性能。因此，与密集奖励相比，使用稀疏奖励时的训练任务更具挑战性，为了应对这一挑战，我们使用 HER 来增强过去的经验。通过重放已实现或即将实现目标的经验，代理将对目标达成行为进行归纳。因此，不成功的经验仍然有助于引导代理，因为稀疏奖励无法提供与预期目标接近程度的反馈。

我们按以下方式实施一般的训练和评估程序：在每个周期内，通过与环境互动收集一个完整的事件，然后进行 1,000 个优化步骤。此外，我们通过对 10 个测试周期进行平均，每隔 5 个周期计算一次成功率。我们使用 ADAM（Kingma 和 Ba，2017 年）作为优化器，学习率为 1e - 5，批量大小为 256。经验重放内存大小设为 800k，存储 10,000 个样本后开始训练。q 网络和策略网络分别由 4 层和 3 层线性层组成，隐藏维度为 256，采用 ReLU（Agarap，2018 年）激活函数。这两个网络的可视化图见补充图 S2、S3。为了更新目标网络，我们设置了 0.005 的更新因子。HER 设定为使用 "未来 "策略，并额外采样 6 次经验（Andrychowicz 等人，2017 年）。所有超参数均为手动调整，并在所有实验中保持固定。

#### SliceIt! - A Dual Simulator Framework for Learning Robot Food Slicing
2024 IEEE International Conference on Robotics and Automation (ICRA)
*开源*

因此，我们提出了 SliceIt！，这是一个在模拟中安全高效地学习机器人食物切片任务的框架。遵循 real2sim2real 方法，我们的框架包括收集一些真实的食物切片数据，校准我们的双仿真环境（高保真切割模拟器和机器人模拟器），在校准的仿真环境中学习合规的控制策略，最后，在真实机器人上部署策略。

我们提出了一种将切割模拟器 （CutSim） 与机器人模拟器 （RoboSim） 相结合的双仿真环境。CutSim 为 RoboSim 提供了更逼真的交互力。同时，RoboSim 将模拟机器人生成的刀运动提供给 CutSim。此外，RoboSim 使用起来更实用，因为生成的动作可以更轻松

在这项工作中，我们提出了 SliceIt！，这是一个用于安全学习机器人切割的框架，具有双重仿真环境。我们的系统遵循真实到模拟到真实 （real2sim2real） [6] 公式，包括 （1） 从切片真实食物中收集的数据。（2） 校准 CutSim 以高保真度模拟收集的切片数据。（3） 使用我们校准的双仿真环境学习控制策略。（4） 在真实的机器人系统上部署策略。图 1 描述了我们提出的方法的概述。

首先，“real2sim” 阶段涉及双仿真环境的数据采集和校准。我们的仿真环境由两个并发模拟器组成：一个为切割软材料量身定制的物理模拟器 （CutSim） 和一个机器人模拟器 （RoboSim）。我们通过让现实世界的机器人切片食物来收集数据。然后利用这些数据来微调切割模拟器的模拟参数。校准只需要少量数据样本。

其次， “sim2real” 阶段侧重于在模拟中使用强化学习 （RL） 训练控制策略，并将其部署到现实世界中。为此，使用了组合式切割模拟器和机器人模拟器。

1） CutSim：在这项工作中，使用了 DiSECt 仿真器。选择 DiSECt 是因为它允许我们通过校准其仿真参数来真实地模拟食品切割。模拟器的可微性使得使用基于梯度的优化方法[4]来校准仿真参数

校准：校准过程包括模拟机器人的切割动作，包括运动和接触力，并调整模拟参数，直到力分布与所需的力分布相匹配。模拟器的可微性使我们能够使用基于梯度的优化方法对这些参数进行微调 [4]。然而，基于梯度的优化可能是计算密集型的，不合适的初始参数有时会导致学习不稳定。因此，在这项研究中，我们提出了一种*两步法。最初，使用非基于梯度的优化方法来快速且经济高效地确定初始仿真参数集。随后，采用基于梯度的优化方法，特别是 Adam 算法 [26]，进一步细化这些仿真参数。为了优化初始仿真参数，我们利用了 Optuna [28] 中实现的树结构 Parzen Estimator 算法 [27]。*

2） RoboSim：我们使用 Gazebo 模拟器，这是一个开源的 3D 机器人模拟器 [5]。我们选择 Gazebo 模拟器的动机是它与机器人作系统 （ROS） [29] 的兼容性。ROS 是一个开源机器人中间件，可促进不同机器人组件的集成。在这项工作中，ROS 用于集成模拟器、真实机器人、我们提出的 RL 控制策略及其低级合规性控制器，如图 2 所示

3） Simulators' Bridge：我们决定使用两个模拟器，是因为 CutSim 的计算成本很高。为了从 CutSim 获得更好的结果，仿真需要以更高的频率运行。在实践中，dt = 1.0e−5 的时间步长给了我们最好的结果。同时， RoboSim 可以以较低的频率运行，因此需要的计算成本要低得多。RoboSim 使用 dt = 1.0e-4 的时间步长。但是，RoboSim 不能考虑切割平面中的相互作用力，它只能考虑对物体表面的法向接触力。在我们提出的框架中，两个模拟器都与具有设定持续时间的交错模拟并行运行。在实践中，我们使用优傲机器人 UR5e 的最高控制频率，即 2.0e-3 秒的持续时间。模拟器通过 ROS 消息交换信息。RoboSim 提供刀相对于机器人的位置 x 和速度 ẋ，而 CutSim 为机器人柔度控制器提供接触力 Fext，如图 2 所示。

我们使用了 RL 算法软演员-评论家 （SAC）。SAC [31] 是一种基于最大熵的非政策参与者-批评者深度 RL 算法。SAC 旨在最大化预期奖励，同时优化最大熵 H。该代理优化最大熵目标，鼓励根据温度参数α进行探索。

2)柔度控制：对于使用刚性机械臂进行食品切片的接触密集型任务，使用了前向动力学柔度控制 （FDCC） 方法 [33]。*FDCC 将阻抗控制、导纳控制和力控制三种控制原理结合到一个新策略中*，以实现笛卡尔柔度控制。作为 FDCC 的关键组件，利用虚拟模型的正向动力学仿真将笛卡尔输入直接映射到联合控制命令，从而在奇点中实现良好的稳定性。我们提出的系统使用 FDCC for ROS 的开源实现 2。

3)RL 代理：传统上，针对特定任务对顺应性控制器进行微调是一个耗时的过程，需要人类的专业知识。为了降低这些要求，我们受先前工作[34]的启发，使用 RL 学习切割动作的运动以及 FDCC 的控制参数。与文献[34]相比，我们使用 FDCC 的原因是它在奇异点中具有更好的稳定性，而且需要学习的参数更少。如图 4 所示，RL 代理的动作以较低的控制频率为 FDCC 提供参考轨迹 xc 和控制参数 [Kc、Kp、Kd]。然后，FDCC 以可用的最高控制频率直接控制机器人的关节指令。来自机器人的反馈信息是根据机器人关节位置的前向运动学计算得出的刀的姿势，以及感应到的力和动量。代理的观察结果包括刀相对于目标位置的位置、速度和抽搐，以及之前采取的行动和传感器的 n 个力-扭矩读数的历史。

SAC 的 actor-network 架构包括一个时间卷积网络 （TCN） [35] 和一个处理剩余观测值的完全连接网络，前者处理来自传感器的 n 个力/扭矩读数的历史。两个网络都输出 64 个特征，这些特征在一个额外的全连接网络上连接和处理，这种结构的简化版本如图 4 所示。网络架构的选择是基于以前工作的结果 [24]。

4） 奖励函数：奖励函数是根据刀相对于砧板 xcut 的高度、接触力 Fext 和刀的生涩 .x...此外，我们还为终止剧集、完成任务（刀在某个阈值内达到所需的目标姿势）、与砧板碰撞（超过最大接触力）、移出定义的工作区以及完成任务时间过长等情况定义了奖励值。

#### Learning Robust Skills for Tightly Coordinated Arms  in Contact-Rich Tasks
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 3, MARCH 2024

通常在工业场景中，不仅需要结构化的环境，还需要精确的位置控制和严格的姿势校准以减少接触。然而，实现这些先决条件在经济上往往很困难，而且在我们的日常生活中也可能被认为是不切实际的。因此，让紧密协调的手臂在接触频繁的任务中熟练地进行互动是非常有益的。

为了实现紧密协调，近几十年来提出了各种基于顺应控制的方法 [2]、[3]、[4]、[5]。这些方法基于被操纵物体和机械臂的耦合动力学，直接或间接地控制内力和外力，从而实现物体与环境之间的灵活夹持和顺应性交互 [2]、[3]、[4]、[5]。然而，耦合动力学建模非常复杂，控制阶段的在线解耦计算量很大。此外，*这些方法只能为手臂提供顺应方式，对于接触丰富的任务，仍需手动编程特定任务技能*。

最近，有人提出了基于强化学习（RL）的方法，用于为接触丰富的任务学习稳健的技能，这为自主训练运动技能提供了潜力，只需有限或无需额外的工程努力[6]，[7]，[8]，[9]。这些研究表明，基于 RL 的方法可以有效利用多模态感知信息，而且学习到的策略可以推广到变化适中的新场景中。然而，这些研究仅限于单臂场景，而针对紧密配合的双臂在接触丰富的任务中学习技能的 RL 方法还有待研究。*与单臂任务相比，多臂任务中的手臂需要在与环境交互时保持协调。*

将 RL 应用于多臂任务的挑战在于样本效率低下 [10]。RL 方法必须处理由于多臂任务的存在而增加的探索负担。为了提高探索效率，大多数研究都试图将人类技能融入学习过程 [10]、[11]、[12]，但这种方法既不灵活又昂贵。最近，*合作多代理强化学习（MARL）*取得了重大进展，价值分解方法达到了最先进的性能[13]。

换句话说，*该政策将严密的协调技能和特定任务技能融为一体*。这样，传统方法所需的大量建模、控制和预编程工作就可以免去了。在这项工作中，我们将整个任务视为一个合作的 MARL 问题，并利用*价值分解*的多代理策略梯度（MAPG）方法来克服样本低效问题，从而使机械臂能够在不整合人类技能的情况下有效地学习技能。

这封信的主要贡献有(1) 我们提出了一个创新框架，并利用价值分解 MAPG 方法来学习紧密协调手臂的稳健策略，从而在不整合人类技能的情况下，在接触丰富的任务中巧妙地操纵物体。(2) 我们提出了一种统一的非策略学习技术，以提高离散和连续动作空间的采样效率。(3) 我们提出了一种名为滑动奖励的新型奖励形式，以避免复杂的奖励塑造，并使其更便于调节手臂的运动。(4) 我们提供了两个双臂装配任务，以证明操纵技能可以在模拟中学习，并且学习到的策略可以直接移植到真实的手臂上。

直观地说，紧密配合是一个保持手臂之间闭链约束的问题。然而，在现实世界中，姿势校准偏差、模型误差和外部干扰等因素可能会导致大量内力或力矩，从而对系统造成潜在损害。为了分别实现灵活夹紧和顺应物体与环境的交互，传统的顺应性控制方法将施加在手臂末端执行器上的力解耦为内力和外力 [2]、[3]、[4]、[5]。Uchiyama 等人提出了一种基于力/位置混合控制的方法 [14]，可同时控制内力和物体的运动，但只集中于自由空间 [2]。Yoshikawa 等人将这种方法扩展到了受限环境，并实现了对外力的控制 [3]。阻抗控制 [15] 是实现顺应性的另一种方法。Caccavale 等人通过在统一的控制框架内结合分散阻抗控制和集中阻抗控制，实现了手臂与物体之间的内部阻抗以及物体与环境之间的外部阻抗 [4]。在同一控制框架内，Ren 等人提出了一种生物仿生阻抗控制策略，以应对复杂和未知的环境干扰 [5]。*这些方法都依赖于被操控物体的动态特性*。实际上，一些动态参数（如物体的加速度）需要估算，而估算往往会产生噪声，且计算量较大。

在这项工作中，我们使用无模型 RL 学习*紧密协调*技能。虽然训练过程需要大量计算资源，但我们的方法无需进行模型解耦和参数估计的在线计算。此外，与大多数传统的遵从性控制方法不同，我们的方法可以实现以下优势，我们的方法学习的几乎是*分布式策略*，除了时间同步外，不需要在臂之间交换任何状态信息。这大大提高了系统的适应性和效率。

最近，RL 已被应用于接触丰富的任务，尤其是装配任务 [6]、[7]、[8]、[9]。Inoue 等人通过 RL 训练了一个递归网络，实现了高精度装配 [6]。Vecerik 等人提出了 DDPGfD，利用人类示范来学习仅有稀疏奖励的插入技能 [7]。Luo 对 DDPGfD 进行了改进，以学习可靠性和鲁棒性更强的工业装配任务 [8]。Lee 等人训练了一种多模态表征，以提高策略学习的样本效率，通过他们的方法学习到的策略可以在不同的几何形状、配置和间隙中通用，同时对外部扰动具有鲁棒性[9]。

对于多臂任务来说，手动编程操作技能非常麻烦，而且往往不切实际。RL 方法提供了自主学习技能的能力，但在多臂设置中面临样本效率低下的问题 [10]、[11]、[12]。Chitnis 等人预先编程了一组参数化技能，然后分别学习任务模式和参数选择策略 [11]。虽然技能参数化提高了学习效率，但仍需要预先编程，并不可避免地限制了技能的多样性。Tung 等人通过行为克隆从大量离线人类演示中学习人类技能[10]。Amadio 等人也从人类演示中学习动作原型 [12]。虽然人类演示提供了大量的人类技能和直觉，但收集这些演示可能是一项繁重的任务。

这些研究利用人类技能来提高学习效率，但这种方法可能不够灵活，而且负担沉重。在这项工作中，我们改进并利用了价值分解 MAPG 方法，在不整合人类技能的情况下缓解了样本效率低下的问题。我们的研究重点是在接触丰富的任务中学习紧密配合的手臂技能，这方面的研究很少。

近年来，合作式 MARL 取得了长足进步，尤其是在基于价值的方法中。基于价值的多代理方法大致可分为集中方法、独立方法和价值分解方法 [16]。集中式方法将 MARL 简化为联合行动空间和全局状态上的单个代理 RL。然而，集中式方法在可扩展性方面存在困难，因为随着代理数量的增加，联合行动空间会呈指数增长。与此相反，独立方法可以直接学习分散的行动值函数和策略，从而轻松实现扩展 [17]。然而，分散学习往往会导致非稳态环境，使学习过程不稳定。此外，分散学习还会导致多代理信用分配问题 [18]。

在这两个极端之间，*价值分解方法使用全局行动价值函数来组合分散的行动价值函数*，既显示了可扩展性和稳定性，又解决了信用分配问题 [16]、[19]、[20]。尽管如此，与政策梯度方法相比，这些基于值的方法在稳定性和收敛性方面仍有很大差距，而在连续行动空间中，这种差距会进一步加剧。最近，有人在策略梯度方法中引入了值分解[13]，从而解决了这些难题，并获得了最先进的性能。

框架由两部分组成：底层遵从控制器和高层策略。具体来说，合规控制器只为机械臂提供合规行为，而策略则整合了紧密协调技能和特定任务技能。策略是分布式的，每个策略将局部观测（o1 ∼ on）作为输入，并输出动作（Δx1 ∼ Δxn），即末端执行器的相对位移。高层策略生成的目标（x1d ∼ xdn）随后由低层顺从控制器执行。

在整个操纵过程中，采用导纳控制器 [21] 作为底层顺应性控制器。导纳控制器由一个内位置控制环和一个外环组成，外环根据目标导纳模型修改参考轨迹以实现顺从行为。目标导纳模型描述了一个质量-弹簧-阻尼系统。当机械臂与环境相互作用时，内部位置控制器会调整并跟踪所需的轨迹。惯性、阻尼和刚度矩阵。

我们将多臂合作任务建模为*分散式部分可观测马尔可夫决策过程*（Dec-POMDP）。局部观测包括从关节编码器获得的*本体感觉信息*和从安装在*手腕上的力-力矩*（F/T）传感器获得的力信息。分散策略首先在仿真中学习，然后直接移植到真实手臂上，无需进一步调整。

将 RL 应用于多臂任务会遇到样本效率低下的挑战。我们使用*价值分解反事实多代理策略梯度（VDCOMA）* 方法[13]来学习多臂操作技能，该方法在许多挑战中都取得了最先进的性能。下一节，我们将介绍 VDCOMA 算法以及我们对该算法的两项改进。

价值分解 COMA：Dec-POMDP 可以由一个元组 〈I，S，A，Ω，P，R，γ〉来定义。在时间 t，每个代理 i∈I 观察 oit∈Ωi ，其中只包含全局状态 st∈S 的部分信息，然后选择一个行动 ait∈Ai 。根据过渡函数 P (st+1|st, at)，其中 at = {at1, at2, ., atn} 是联合行动，代理将进入新的状态 st+1，团队奖励为 rt = R（st，at）。每个代理学习随机策略 πi(ait | oit; θi)，以最大化预期累积贴现奖励 ∑∞ t=0 γtrt，其中 γ∈ [0, 1) 是折扣因子。

on-policy和off-policy方法的结合

滑动奖励：
模仿参考运动是利用先验知识和简化奖励塑造的直接方法[24]。从本质上讲，我们提出的滑动奖励是一种具有滑动机制的模仿奖励，它减少了对参考运动的依赖。滑动机制消除了精确参考运动规划的必要性，包括无需考虑手臂的操纵顺序、手臂之间的闭链约束无需严格遵守、目标位置无需高度精确等。以下是我们的滑动奖励设计的具体细节。

其中，T i 代表机械臂 i 的参考轨迹，pi1、pi2、......、pik ∈ R3 是 T i 上的连续点。参考轨迹为每个机械臂提供了缩小探索空间的粗略指导，无需严格遵守闭链约束。这些引导轨迹是学习紧密协调技能和特定任务技能的基础。此外，由于在学习过程中会模仿参考轨迹，因此可以通过调整参考轨迹来方便地调节手臂的运动。

根据规划的参考轨迹设计模仿目标至关重要。对于单臂任务，通常的做法是将模仿目标设定为当前位置与相应参考位置之间距离的函数。然而，对于多臂任务，在每个时间步将每个手臂的位置与固定的参考位置绑定可能会限制过多。如果参考轨迹不严格符合闭链约束条件，就会对协调技能的学习产生不利影响。因此，我们不模仿固定的参考位置，而是采用滑动窗口来放松对每只手臂的限制。

其中，w 是滑动窗口的大小，xit ∈ R3 是第 i 个手臂末端执行器在时间 t 时的笛卡尔位置。

我们设计了两个双臂装配任务来评估我们的方法。图 1 展示了这两个任务的细节。一个是刚性装配任务，即双臂夹住一个刚性工件并将其插入两侧的孔中。与广泛研究的单臂孔中钉任务相比，刚性装配任务更具挑战性，因为在装配过程中需要协调。另一种是*软装配任务*。同样，需要将刚性工件装入两侧的槽中，槽由两根交错的弹性纤维支撑。为了完成任务，手臂需要在操作过程中通过弯曲弹性纤维来对齐两个插槽。

考虑到真实世界与模拟之间的姿态偏差，我们在训练过程中为孔或槽的标称姿态添加了一个很小的零均值高斯噪声。如图 4(c)和(d)所示，噪声被添加到贴在孔或槽上的任务坐标系 ΣT 的姿态上。

每个臂的观测值：其中，pi ∈ R3、qi ∈ R4 分别为第 i 个机械臂末端执行器的位置和方向，可在实际实验中从机械臂的控制盒中获取；f i ∈ R6 表示施加在第 i 个机械臂末端执行器上的外部扳手，由配备的 F/T 传感器测量。其中，pobj ∈ R3、qobj ∈ R4 是被操纵物体的位置和方向，可从机械手的姿态推导得出。为了实现从模拟到现实的直接传输，上述所有量都在任务坐标系中表示。

当施加到末端执行器上的外部扳手超过上限值或机械臂无法抓住工件时停止探索

动作空间：在随后的消融研究中，将比较离散和连续动作空间的学习效率。连续动作空间定义为[Δx, Δy, Δz]，其中Δx, Δy, Δz∈R 是沿坐标轴的位移。离散动作空间由方向集 {±X, ±Y, ±Z, XZ1∼4} 和单位步长组成，其中 XZ1∼4 代表 XZ 平面上的四个对角线方向，这是为了增强灵活性而添加的。

在这两项任务中，离散动作空间的学习效率要比连续动作空间高得多。直观地说，由于每一步的步长和方向可以任意选择，手臂在连续行动空间中学习技能更加灵活。因此，我们需要在训练负担和策略灵活性之间做出权衡。有趣的是，我们发现在离散行动空间中学习到的策略可以适应不同的步长，只要步长小于训练阶段的设置即可。图 6 显示了采用相同策略的两种不同步长所产生的轨迹。学习到的策略能够适应不同的步长，主要得益于长期训练过程中的广泛探索。广泛的探索使该策略在大多数状态下都能选择正确的方向，从而减轻了步长的重要性。

就学习效率而言，模仿参考轨迹有助于减少训练过程中的无意义探索。从另一个角度看，参考轨迹也有助于我们约束手臂的运动。如图 6 所示，真实轨迹与参考轨迹非常接近。通过适当调整参考轨迹，可以方便地调节手臂的运动。

2)模拟到现实的转换：为减少现实世界与模拟之间的差距，我们采取了两项措施。首先，所有与观测和状态相关的量都在任务坐标系中表示。因此，只要工件与装配环境的相对位置保持不变，就可以改变工件的初始姿态。其次，在训练过程中为孔/槽引入姿态噪声。这样，学习到的策略对微小的姿势变化和真实世界与模拟之间的接触模型差距具有鲁棒性。

#### A reinforcement learning based control framework  for robot gear assembly with demonstration learning  and force feedback

强化学习 （RL） 已在机器人作任务中得到广泛应用，尤其是在涉及大量接触活动（如轴孔装配和点焊）的场景中。然而，现有方法遇到了几个关键限制：难以收集真实世界的实验数据，奖励收敛缓慢导致策略表现欠佳，以及模拟与现实之间的显着差异。本文介绍了一种新的框架，旨在克服机器人齿轮装配背景下的这些限制。最初，我们通过人工指导捕获由人类专家执行的装配轨迹，然后将这些专家经验纳入经验池。利用深度确定性策略梯度 （DDPG） 算法，以及演示、力反馈和域随机化，我们训练策略，直到在仿真中取得令人满意的结果。我们在 MuJoCo 模拟器中建立仿真环境，并在实际齿轮轴装配任务中验证所提出的方法。实验结果展示了我们的框架在成功完成组装任务方面的稳健性和有效性。

与人工装配相比，机器人装配提供了更高的效率和精度，将工人从重复、繁重的任务中解放出来。特别是，钉入孔组装 （PiH） 是最具代表性的组装任务 [1]。如图 1 所示，PiH 组装过程主要分为三个阶段：查找、对齐和插入。但是，该任务面临许多技术挑战。机器人需要实现精确的位置和力控制，以确保钉子的准确对准和与孔的稳定连接。传统方法严重依赖机器人的位置控制精度和传感器反馈。

这种方法通常需要昂贵的传感器设备，难以适应复杂和动态的装配环境，并且表现出有限的技能泛化。此外，制定精确的轨迹规划需要大量的编程和示教时间 [2]，这限制了其在实际应用中的灵活性和适应性。找到学习机器人 PiH 组装技能的方法所必需的。

在本文中，我们旨在学习可以完成高精度齿轮轴装配任务的策略，如图 2 所示。在实际制造中，人工可以相当轻松地完成如此高精度的复杂任务。这是因为人类会“感觉到”钉子插入孔中的接触点 [3]。RL 有望使代理能够通过与环境的交互来学习行为，理想情况下，它们能够泛化到新颖的、看不见的场景 [4] [5]。受此启发，我们将 RL 与专家演示和末端力反馈相结合，以解决 Pih 任务。然而，强化学习方法很容易陷入局部最优和非收敛问题。我们试图回答以下三个问题：

1）如何轻松获得专家演示，通过演示训练策略？2） 我们能否通过明确考虑力/扭矩测量来学习稳健的装配策略？3） 我们的框架是否适用于现实世界的任务和机器人？

本文的贡献就是对这些问题的回答。首先，我们通过拉动机器人的末端来完成齿轮轴组装任务，得到*演示的轨迹*。其次，我们将力/扭矩传感器数据合并到状态空间中，并设计一个*包含力信息的奖励函数*。第三，我们使用 RL 算法与演示、力反馈和域随机化训练策略，直到在模拟中取得良好的结果。训练后，策略给出的动作命令被发送到真实的机器人执行。真实世界和模拟环境中的实验结果表明，我们提出的方法具有出色的性能和效率。

现代工业制造场景中的许多任务，如组装 [6]、抛光、推送、切割 [7] 等，都需要机器人末端执行器 （EE） 或工具与环境接触，称为多接触作。因此，在学习过程中进行探索作可能会对机器人身体及其环境造成伤害 [8]。力反馈是多点触控作中最重要的方式之一，用于微调物体运动并确保安全 [9]。RL 需要大量的训练，这通常需要收集大量的实验数据。在现实中，实验样本采集效率低，数据采集困难，浪费时间 [10]。一些学者提出构建仿真环境，建立接触对象的碰撞模型，并使用 GPU 来缩短强化学习算法的训练时间 [11] [12]。为响应 RL 在机器人领域的快速发展，开发了许多模拟器，例如 Vrep [13]、Pybullet [14]、MuJoCo [15]、Isaac 等。其中，MuJoCo 是一款具有非常逼真的物理引擎的模拟器，并提供了大量的物理计算接口，以方便仿真训练和学习。由于这些优势，我们在这项工作中也使用 mujoco 进行 RL 训练。例如，Stepputtis等[16]使用Mujoco仿真环境提出了一个接触丰富的双机装配仿真学习框架，以完成墙板的多轴孔装配任务。

随着人工智能技术的飞速发展，机器学习算法在机器人作领域取得了长足的进步和应用 [17]。目前，装配领域使用的机器学习方法主要有两种类型是模仿学习和强化学习。模仿学习避免了通过专家演示从零开始学习技能，已成为机器人作的有效方法之一 [16] [18] 。但是模仿学习并不能对丰富的感官输入进行反馈。在装配任务中，视觉信息用于获取目标物体的位置和姿态，力反馈主要用于微调物体的运动并确保安全 [19]。与模仿学习相比，强化学习没有这些限制，可以基于奖励函数在空间内探索最优策略。目前应用于机器人领域的主要 RL 算法是 DQN [20]、DDPG [21]、PPO [22] 和 SAC [23]。在最近的工作中，演示已被用于策略的预训练，例如 DQfD [24]。使用专家的轨迹进行预训练 RL 策略有助于加快训练速度。有一种简单的方法可以将 RL 与 Demonstrations 相结合，这些 Demonstrations 被整合到 RL 算法中 [12]。我们还采用了类似的方法，将演示数据添加到重放缓冲区中，用于非策略 RL 训练。

为了解决齿轮组装任务，我们提出了一个框架，让机器人学习组装技能，如图 3 所示。主要包括收集演示数据和 RL 训练，这将在以下小节中讨论。

强化学习：我们将问题概念化为马尔可夫决策过程，表示为元组：（S， A， R， P， γ），其中代理在每个离散时间步 t 都处于状态 st ∈ S，在∈ A 处执行一个动作，获得标量奖励 rt （st， at） ，环境根据转换概率 p （st+1 | st， at） 演化到下一个状态。强化学习的目标是在 ∽ π （at | st） 处选择作，以最大化预期回报 R = PH t=0 γtrt，其中 H 是视野，γ ∈ （0， 1） 是折扣因子。

演示的DDPG：演示中的 RL 在复杂任务中表现出更好的性能。如图 4 所示，我们通过手动指导原理图示教获得了人类专家的经验，其中包括在记录机械臂的关节角度数据的同时拖放机械臂的末端进行齿轮组装。然后，通过反向运动学将关节角度数据转换为末端执行器运动轨迹。RL 格式的演示数据存储在包含状态、作、奖励和下一个状态数据的重放缓冲池中。

选择非策略 RL 算法可以缓解策略遗忘问题。因此，我们选择了深度确定性策略梯度 （DDPG） [21] 作为我们的 DRL 算法。DDPG 是一种无模型、非策略、参与者批评者 RL 算法，旨在学习连续动作。它融合了 DPG （确定性策略梯度） 和 DQN （深度 Q 网络） 的概念。它结合了 DQN 的 “经验重放 ”和缓慢学习的目标网络，在连续动作空间中有效运行，并建立在 DPG 原则之上。环境通过执行作 at+1 = π （st | θπ） + N ，生成新的转换 e = （st， at， rt， st+1），其中 N 是一个允许作探索的随机过程。然后，我们将这些新的过渡添加到 replay 缓冲区中。在训练期间，我们对一批数据进行采样以计算损失：

使用力反馈的 RL 训练：
在组装过程中，由于严格的精度要求和细长的轴，经常会出现卡住问题。为了应对这一挑战，机器人的末端集成了力传感器。状态空间 （St） 和动作空间 （At） 定义如下：
St = [xg， yg， zg， xs， ys， zs， fx， fy， fz]
At = [δx， δy， δz]
其中 （xg， yg， zg） 是齿轮在时间 t 的位置，（xs， ys， zs） 表示轴的目标位置。（FX， FY， FZ） 是末端执行器的三维力信息，（δx， δy， δz） 是末端执行器的位移。

域随机化 （DR） 是提高训练模型的鲁棒性和 sim 到 real 转移成功率的有效方法 [11]。在训练时，我们在参数和真值位置添加了噪声。在我们的框架中，DR 在纠错方面效果很好。

在我们的实验中，我们专注于回答以下问题： • 通过手动指导原理图教学添加的专家经验有什么效果。• 添加力信息是否有助于装配过程？• 我们的框架是否适用于实际的汇编任务？

A. 实验设置
计算机环境在 Ubuntu 20.04 LTS 64 位平台上运行，具有 Intel（R） Core（TM） i5-13600K 处理器和 GeForce RTX 3080 GPU。这些任务是通过 Rethink Robotics 的 Sawyer 7 自由度 （DOF） 扭矩控制臂完成的。我们利用 Mujoco 模拟器 [15] 来模拟 Sawyer 机器人，采用公开可用的运动学和网格文件。在模拟和实际实验中，入的齿轮牢固地连接到抓手上，而轴仍然固定在桌面上。如图 2（b） 所示，齿轮的外径分别为 40mm 和 30mm，而两个内径均为 10mm。轴的直径为 8 毫米。我们用设计的 PID 控制器实现机器人的联合控制。如图 5 所示，在模拟中，作由策略决定为机器人末端执行器位置的增量变化。利用 URDF 文件导入机器人的参数，获得末端执行器位置，并采用机器人的逆运动学来计算所需的关节角度。此外，PID 控制器将这些作转换为虚拟电机的命令。我们还使用 Open AI Gym 和 Pytorch 开发 RL 算法，后者被用作 RL 算法的便捷工具包。

我们收集了 100 集人工演示，每集长达 64 步。然后，我们将这 100 集的经验以适合强化学习训练的格式存储到体验重播缓冲区中。我们使用 DDPG [21] 作为 RL 算法来训练智能体完成齿轮轴装配任务。在训练开始之前，我们对轴的位置有一个初步的估计。在训练过程中，我们采用了两种领域随机化方法：观察随机化和动态随机化。观察随机化表示传感器中的误差，而动态随机化表示现实世界中机器人控制的不确定性。噪声通常被描述为加法或缩放噪声，从高斯分布 N ∽ （μ， ρ） 中采样。实验配置如表 I 所示。我们用 1000 个事件训练了策略，每个事件在域随机化下持续 64 个步骤。

结果表明，DDPGfD 优于 DDPG，表现出更好的训练行为。*DDPGfD 通过整合专家经验来提高数据效率和训练速度。* 我们还采用 PPO 作为 RL 算法来评估力反馈在齿轮轴装配任务中的影响。添加了力信息的奖励函数由公式 （12） 表示，而没有力信息的奖励函数为：rtotal = r1 + r3。如图 10 所示，带有力反馈的奖励函数收敛到更高的最终值。这表明合并力信息可以有效地解决轴孔装配任务中的局部最优和干扰问题。但是，PPO 算法在训练过程中偶尔会出现过拟合。

最后，我们在实际场景中实现了 DDPGfD 训练的程序集策略。在实验之前，真实和模拟环境中的轴都精确定位在全局坐标内，而机器人共享相同的初始关节角度。在组装过程中，策略生成的命令通过 ROS 传送给实际的机器人执行。如图 6 和图 7 所示，实验结果表明，我们提出的方法使机器人能够在现实世界中完成组装任务。

在本文中，我们将强化学习与演示和力反馈相结合，以解决高精度装配问题。我们证明，我们的框架是在模拟和真实环境中解决齿轮轴装配任务的有效方法。通过*专家演示*，强化学习算法可以推广到更稳健的策略，并更快地获得最佳策略。我们还引入了*末端执行器的力反馈*，*并在状态空间和奖励函数中考虑力/扭矩信息*，以避免组装过程中的局部最优和卡住。我们还成功安全地将策略从模拟器转移到现实世界。对于未来的工作，我们将在此基础上通过研究灵巧的手臂-手系统的纵来构建。我们将研究更复杂的作和对象，并为不同的作任务开发更通用的框架。








#### Using Goal-Conditioned Reinforcement Learning  With Deep Imitation to Control Robot Arm in  Flexible Flat Cable Assembly Task

在装配过程中，我们不难发现，即使是最熟练的工人也无法一次性完成这项装配任务。工人在按下手指时，并不会固执地观察公母接头是否对齐，而是通过触觉来调整手指。换句话说，视觉感知用于提供母接头的大致位置，而触觉感知则是精确卡合的基础。使用机械臂来完成 FFC 组装任务，现有作品很难实现完全自主控制并完全取代人工。


#### Reinforcement Learning Based Variable Impedance Control for High  Precision Human-robot Collaboration Tasks

人机协作是智能制造中一个具有巨大潜力的重要领域。由于协作任务的多样性，机器人协作技能应具备适应不同技能的能力。然而，技能表达和泛化等问题都具有挑战性。同时，不同操作员的技能差异也给协作机器人带来了困难。本研究开发了一种用于人机协作装配的可变阻抗学习方法。与以往大多数主要讨论具有固定阻抗参数的特殊人类协作者的研究不同，本研究通过强化学习来学习机器人阻抗。我们的目标是通过近端策略优化（PPO）算法使惯性、阻尼和刚度参数自适应。因此，我们可以让机器人与不同的人类协作完成高精度的装配任务。两个实验结果说明了所提方法的有效性。详细实验视频请访问 https://youtu.be/AJyjW2NwA74

装配是工业机器人作的关键和基本部分。传统的组装通常由机器人完成，没有任何参与，然而，人机协作在许多工业应用中显示出其广阔的前景。人机协作是工业 4.0 背景下工业机器人发展的一个重要方面。协作机器人在协作装配和协作运输方面显示出巨大的发展潜力 [1][2]。这是因为人类和协作机器人的优势都可以在人机协作的过程中得到利用，从而提高生产率。

长期愿景是使机器人足够安全和灵活，以便在工业生产线上与人类合作，人类和机器人在合作任务中都扮演着控制者的角色。

一些早期的工作依赖于作员直接拉动机械臂[3]或采用示教方法，即机器人向人类教练学习技能，通常包括三个步骤：教学、数据建模和技能复制。常用的模型包括动力运动基元 （DMP） [4]、高斯混合模型 （GMM） [5]、隐马尔可夫模型 （HMM） [6]、高斯过程回归模型 （GPR） [7]。例如，[8] 使用了 Gaussian混合模型 （GMM） 和高斯混合回归 （GMR） 分别用于编码和再现机器人协作行为。但缺点是解可能会落入局部极值。此外，GMM 在高维系统下训练难度大，当具有高斯核函数时，计算效率低下。

我们注意到，之前的许多工作都集中在处理重物的人机协作上。例如，B. Nemec 等人[1] 提出了一个使用动态运动基元和迭代学习来关闭盒子盖的框架，但是，此类任务不需要高精度。在另一种情况下，L. Rozo 等人 [9] 提议用任务参数化的高斯混合模型对人类教师的演示进行概率编码，并在桌子组装任务上进行演示，机器人从人类演示中学习协作技能，这很乏味，而且不可避免地有其局限性。

#### 前向动力学柔顺控制器FDCC

机器人技术中的顺应性末端效应器是物体操纵和环境交互领域的重要先决条件。然而，目前的机械手通常是僵硬的位置控制系统，因此需要增加顺应性。在这项工作中，我们提出了前向动力学顺应控制（FDCC），这是一种全新的三重控制概念，通过将导纳、阻抗和力控制结合到一个控制策略中来实现笛卡尔顺应性。我们仅通过力矩传感器来闭合控制环路，从而实现了与系统无关的终端效应器顺应性解耦配置。作为 FDCC 的关键组成部分，我们利用虚拟模型的前向动力学模拟，将笛卡尔输入直接映射到关节控制指令，从而在奇异情况下实现出色的稳定性。在三种不同机器人操纵器上进行的实验验证了这种方法的关键优势。

机器人机械手必须为产生接触力做好准备，因为它们会出现在各种以接触为主的任务中，例如各种物体操作。然而，尽管有这一至关重要的必要性，绝大多数已部署的机械手都是具有高增益位置控制的刚性系统。为了以附加元件的形式在这些系统上实现顺从行为，人们进行了大量的研究。在这项工作中，我们使用顺应性一词来泛指机器人机械手承受外力的能力。有一些固有的顺应性机械装置可以实现这种行为，例如早期实现的远程中心顺应性（RCC）[1]，也称为被动顺应性，以及各种最新的实现[2]。在软件控制中也有各种实现方式，它们构成了模仿某种机械行为的外加控制，最初被称为主动顺应[3]。在这些控制原理中，接触力大多通过力控制[3]、位置/力混合控制[4][5]、导纳控制[6][7]直接调节，或通过阻抗控制[6]和刚度控制[8]隐式调节。

力控制和混合位置/力控制在不同的子空间中处理力度和运动，而阻抗和导纳控制则秉承了运动和力度相互交织的理念，同时也是最好的控制器。通常，与位置、速度和加速度相关的误差补偿用于反映弹簧、阻尼器和质量的行为，并包含线性二阶方程来描述笛卡尔空间中的预期行为。虚拟模型控制（VMC）[9] 对这方面进行了概括，它也将虚拟元素附加到系统中，通过求解方程获得扭矩控制指令。特别是最近关于阻抗控制的研究取得了重大成果[10][11]，并已达到工业成熟阶段[12][13]。

然而，位置控制系统仍然是目前大多数机器人机械手的主流，这些方法对它们并不直接适用。在这项工作中，我们提出了一种新的附加控制策略，以扩展这些经典机器人的功能，如图 1 所示。我们将位置控制视为一个黑盒子，它是与系统交互的最低层级，可在外部控制环中施加某种行为。由于其他控制原理要求机器人具有关节扭矩接口，因此基于计算扭矩法[14][15]的方法并不适用。

为了说明这个问题，让我们以一维线性弹簧阻尼器质量元件的运动方程作为笛卡尔遵从的起点，来表达末端效应器目标运动 x0 和外力 F ext 之间的关系：
F ext = m(x ̈ - x ̈0) + d(x ̈ - x ̈0) + c(x - x0)

这里的目标是计算随时间变化的运动 x(t)，以控制施加在系统上的力（导纳控制）。在一般情况下，一组关节运动 q(t)必须找到，这将影响这个 6D 运动 x(t)，这实质上是从反向角度试图解决这个问题。

然而，所有依赖逆运动学的解决方案都必须采取特殊措施，例如，参见[16][17]，以确保奇点附近的稳定性。否则，系统就会不稳定[10]。一般的阻尼方法[18][19]可以调节广泛的关节速度，但这些方法无法弥补固有的问题。

这就需要改变视角，这就是为什么我们抛开逆运动学，转而建立在快进动力学模拟的基础上，创造出融合多种控制概念的选择。

在我们的方法中，我们将三种控制原理（即阻抗控制、导纳控制和力控制）合并为一种新的控制策略。与 [20] 的方法不同的是，我们并不根据不同的标准在导纳控制或阻抗控制之间进行切换，而是将这两种原理合并在一起。这种新控制策略的核心是，我们利用动力学模拟，通过作用在机器人末端效应器上的虚拟力和测量力直接控制机器人机械手。作为这一概念的关键组成部分，我们提出并评估了前向动力学的使用方法，将其作为一种非常适合的求解器，用于将任务空间中的努力映射到关节空间中的运动指令，并实现所有三种控制概念的轻松整合。通过将仿真作为控制的关键部分，我们接受了 "系统将如何行动 "的理念，这也是软件所能达到的与机械、被动设备最接近的合规性。

组合控制结构
我们通过将这三个控制概念合并到前向动力学求解器的共同接口上，实现了这三个控制概念的组合。整个概念如图 4 所示。这种方法同时采用开环和闭环控制，根据机器人机械手是处于自由运动状态还是与环境接触状态，这两种控制方法本身都适用。闭环控制由导纳控制（Admittance Control）负责，它利用前向动力学求解器来规避逆运动学问题，并通过测量末端效应器上的力矩传感器来计算反作用力运动。这些反作用力运动直接在阻抗控制中进行处理，阻抗控制有意忽略真实机器人系统的关节状态反馈，而是将模拟运动与所需的运动输入进行比较。通过虚拟弹簧、阻尼器和质量，该运动被映射为可调节的 6D 力，并与传感器的测量结果叠加。

在无约束运动中，这将导致不受影响的系统行为，并涵盖开环控制，而在机器人机械手与环境接触时，传感器的测量结果将闭环控制。在第三个控制组件 "力控制 "中，除了上述力之外，还施加了一个 6D 目标力。出现在末端效应器上的所有力和力矩被合并成一个净力，并通过应用前向动力学仿真引起系统的反作用力运动，如图 4 所示的通用求解器管道。这一迭代过程单独运行，完全脱离实际机器人的位置控制循环。然后，模拟运动将直接用于机器人位置控制，并遵循系统特定的控制周期，该周期通常低于 FDCC 采样率。

根据正向动力学模拟的性质，我们首先会得到加速度级的反作用运动，经过两次积分后，我们会得到机械手关节的平滑控制指令，因此使用噪声滤波器是多余的。前向动力学求解器的所有输入均为外力 F ext = F c，每个维度的 PD 控制器都将其控制为零。在理想情况下，净力 F net（包括目标力、传感器测得的力以及阻抗控制中虚拟运动产生的所有力）被控制为零，系统始终处于能量平衡状态，包括在运动过程中和可能与环境接触时。对于 PD 控制器，其中 kp,i 和 kd,i 分别为比例增益和导数增益。需要注意的是，不需要积分增益，因为期望姿态 xd 和虚拟姿态 x 之间的任何偏移都会在虚拟弹簧中产生一个恢复力，从而导致虚拟模型中的修正运动。只要机械手的运动模型足够精确，这种控制就能消除真实系统的稳态误差。

为了在任务空间中描述机器人的任务，系统必须提供用户界面，用于接收指令，同时保持末端效应器的顺从行为。如图 5 所示，FDCC 提供的接口可同时发出与任意末端效应器坐标系 {x, y, z} 有关的所需运动 xd 和所需力 F d 的指令。这可以通过虚拟阻抗控制和导纳控制来实现。可以通过虚拟惯性、阻尼器和弹簧的参数来调整顺从行为的特性。

一个理想的笛卡尔机器人控制装置试图在其所有六个维度上做到完全一致。然而，终端效应器总是属于具有约束条件的运动学链。特别是奇点显示了从任务空间到关节空间映射的强烈非线性。我们的目标是创建一个尽可能独立于关节配置的解决方案，但如果运动学结构无法解释关节空间的运动，则不在关节空间强制执行给定笛卡尔运动的解决方案。

要做到这一点，我们必须以我们想要控制的机器人机械手的精确运动学作为起点。这对于确保真实系统能在理论上执行模拟运动是非常必要的。第二步包含了本文提出的控制方案背后的核心理念：我们建立了一个动态模型，同时将所有环节的惯性降至最低。如图 6 所示，只有最后一个环节被赋予质量和惯性。(2) 至 (4) 中的参数 c、k 和 I 可以自由设置，以反映任何想要的行为。这样做的目的是将链条其余部分可能产生的所有动态影响降至最低，使末端效应器成为控制模拟运动的主要元素。

位置控制式机械手本身就考虑到了重力，因此外部控制回路无需补偿这些影响。为了使虚拟模型不发生漂移，重力被从系统中移除，这与 (3) 中与阻尼器相关的力相结合，形成了一个刚体，与一组无质量链接和关节相连，漂浮在粘性流体中。然后，我们只需对该模型施加整体作用力，计算其反作用运动，并将其作为控制指令下达给机器人机械手。前向动力学求解器的输出是关节空间的加速度，代表受控净力引起的反作用运动。首先获得加速度，就能在积分后获得平滑的速度和位置。此外，集中惯性可作为低通滤波器，对传感器噪声具有鲁棒性。因此，使用前向动力学既能实现高灵敏度，同时又能抑制传感器噪声。

此外，在奇异点中，外力失去了在虚拟模型关节中产生运动的直接作用。此时，虚拟机械手链接的（无限）刚性机械结构将承担外部负载。正交关节轴上不会产生加速度。在这方面，这里介绍的解决方案与逆运动学方法截然相反：在奇异运动学方法中，运动学方法会导致较大的关节运动，而正向动力学方法则完全不会导致关节运动，因此具有更高的稳定性。  需要注意的是，没有必要建立与真实系统接近的动力学模型。事实上，选择完全独立于实际机器人质量的末端效应器集中质量更有优势，这样在大多数工作空间配置中，使用不同的机器人可以获得几乎相同的行为。

操纵器诊断行为
如前所述，涉及真实机械手的控制回路仅通过末端效应器的力矩传感器闭合。由于硬件关节的直接反馈不会产生直接影响，因此控制器的设计可以与实际系统脱钩，具有几乎任意的特性。这样做的好处是，无论底层操纵器如何，都可以结合环境预先调整周期阻尼，从而使我们在此提出的顺应性成为真正的附加顺应性。

在本文中，我们提出了前向动力学顺从控制（FDCC）作为一种新方法，通过在力-导纳组合控制的基础上集成虚拟阻抗控制，实现位置控制机械手的笛卡尔附加顺从性。

我们展示了如何利用阻抗组件来设计和参数化所需的终端效应器顺应性，而不依赖于底层硬件。事实证明，阻抗控制组件在与软环境（人体手臂）和硬环境（工作台表面）接触时具有接触稳定性和顺畅的交互性，并在运动过程中成功施加了目标力。

为了实现所需的笛卡尔运动，我们采用了与传统方法相反的方法，将正向动力学整合到控制环中，为逆运动学问题提供了另一种解决方案。我们的中心思想是模拟一个通用虚拟系统在虚拟和测量的终端效应器力作用下的反作用运动，这使得三种控制概念的融合成为可能。这一新原理使得 FDCC 在奇异情况下表现出平滑、稳健的特性，从而成为增强具有笛卡尔顺应性的位置控制机械手的有力工具。它可用于物体操纵和环境交互，例如在制造单元或装配线中，在这些地方位置控制的机器人很常见。未来的研究可以在 FDCC 的基础上扩展高级操纵策略，利用三个不同的界面同时发出运动、力和顺应指令。