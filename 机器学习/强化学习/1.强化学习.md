# 基础
## 结构
- 基本元素：Agent、Environment、Goal
- 主要元素：State、Action、Reward
- 核心元素：Policy、Value
## 关系
- State 由 Agent 和 Environment 组成
- Action 改变 State
- Reward 是在某个 State 下进行某个 Action 所得到的反馈，由最终的 Goal 决定
- Policy 是指在某个 State 下应该如何选择 Action
- Value 是衡量一个 State 或 State-Action 好坏的标准
## 价值函数（Value Function）
强化学习核心就是学习一个好的价值函数（Value Function）
- 状态价值（State-Value）函数：输入是状态，输出是未来预期奖励之和
- 状态行动价值（State-Action-Value）函数：输入是状态与行动对，输出是未来预期奖励之和
## 特点
- 试错（Trial and Error）
- 延迟奖励（Delayed Reward）
## 关键问题
*Exploration vs. Exploitation 探索与利用*
不仅需要利用已经学习到的兼职函数，还应该去尝试不同的行动（$\epsilon$-Greedy）
# 马尔科夫决策过程（MDP）
[马尔科夫决策过程](https://zhuanlan.zhihu.com/p/359626297)
强化学习问题通常被建模为一个马尔可夫决策过程（Markov Decision Process, MDP），MDP 是一个五元组(S,A,P,R,γ)，其中：
- S：*状态空间*。
- A：*动作空间*。
- P(s′|s,a)：*状态转移概率*，表示在状态s下采取动作a转移到状态s′的概率。
- R(s,a)：*奖励函数*，表示在状态s下采取动作a所获得的奖励。
- γ：*折扣因子*，表示未来奖励的衰减程度。
在 MDP 中，未来的状态只取决于当前的状态和动作，而与之前的状态无关，这就是所谓的马尔可夫性。
# 贝尔曼方程
[贝尔曼方程](https://blog.csdn.net/xzs1210652636/article/details/145444766)
用于递归分解状态或动作的价值，将当前决策的即时奖励和未来状态的长期价值相结合
- 一个状态（或动作）的价值 = 即时奖励 + 折扣因子 * 未来奖励的期望
- 状态价值函数$V^\pi(s)$：表示在当前策略下，从状态s出发的长期预期累计奖励
- 动作价值函数$Q^\pi(s,a)$：表示在当前策略下，从状态s执行动作a后出发的长期预期累计奖励
- 折扣因子$\gamma$：权衡即时奖励与未来奖励（一般小于1）
## 基本形式
状态价值函数的贝尔曼方程：
$$
V^\pi(s) = \mathbb{E}_\pi \left[ R(s， a) + \gamma \sum_{s'} P(s' \mid s， a) V^\pi(s') \right]
$$
动作价值函数的贝尔曼方程：
$$
Q^\pi(s,a) = R(s,a)+\gamma \mathbb{E}_\pi \left[\sum_{s'} P(s' \mid s， a)\sum_{a'}\pi(a'\mid s') Q^\pi(s',a') \right]
$$
$\pi(a\mid s)$是某状态下选择动作的策略，$p(s'\mid s,a)$是状态转移概率
## 最优贝尔曼方程
即最优策略$\pi^*$对应的价值函数满足最大化条件
$$
 V^*(s) = \max_a \left( R(s， a) + \gamma \sum_{s'} P(s' \mid s， a) V^*(s') \right) 
$$
$$
Q^*(s,a) = R(s,a)+\gamma \sum_{s'} P(s' \mid s， a)\max_{a'}Q^*(s',a')
$$
此方程通过动态规划（如值迭代、策略迭代）求解，确定未来可能的最大化回报的策略
在有限状态空间下可以转化为矩阵形式
$$
\mathbf{v}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{v}^\pi
$$
## 参数更新
[详细参数更新过程参考](https://blog.csdn.net/xzs1210652636/article/details/145444766)
# on-policy 和 off-policy
![](https://pic3.zhimg.com/v2-5149f6651f44db908c6fa842c474ee24_1440w.jpg)
主要区别在于
1. 行为策略：如何从环境中生成数据
2. 目标策略：控制智能体的策略（强化学习的目的就是学习控制智能体取得最大收益的策略函数）
直观来讲，on-policy的经验是由正在学习的策略产生的，而off-policy的经验则可以是来自其策略或者过去的策略
## on-policy（使用相同的行为策略和目标策略）
智能体学习过程中使用的样本数据，直接由当前正在优化的策略生成，这些数据用来更新当前策略本身，所以目标策略就是行为策略
这种方法使得学习过程紧密地与当前策略相关联，每一步学习都直接基于当前策略所产生的经验
![](https://i-blog.csdnimg.cn/direct/e8148b1e90ad40ad809cb4737e9d51d2.png#pic_center)
示例算法：策略梯度算法，SARSA（State-Action-Reward-State-Action）、A2C（Advantage-Actor-Critic）、A3C等
特点：
1. 数据与策略绑定，只能从当前策略生成的数据中学习
2. 学习过程稳定，因为学习目标与采样数据来自同一个策略
3. 探索效率较低，数据利用率差，因为只能使用“当前策略”的数据
## off-policy（使用不同的行为策略和目标策略）
使用一个与当前正在学习的策略不同的策略来生成用于学习的数据，所以存在一个行为策略和一个目标策略。利用已有的数据学习一个最优策略，但这些数据不一定有当前正在学习的最优策略产生，可以从这些*离线的、历史的*数据中提取出有用的信息优化目标策略
![](https://i-blog.csdnimg.cn/direct/117e73220e3b41a983a041abaa91a6d1.png#pic_center)
示例算法：Q-Learning、DQN、DDPG等
特点：
1. 数据和策略解耦，可以使用来自其他策略（甚至随机策略）的数据来更新目标策略
2. 数据利用率高，可以复用来自过去或其他策略的数据
3. 学习过程可能不稳定，因为行为策略与目标策略不同



GAIL