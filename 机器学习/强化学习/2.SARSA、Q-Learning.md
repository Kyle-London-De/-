# Q-Learning
Q-Learning 是一种基于值的强化学习算法，通过学习状态-动作值函数（Q 函数）来找到最优策略，*最终得到一张Q表，记录每个状态下进行每个行动的Q值*
Q函数主要分为状态价值函数（State Value）和状态行动价值函数（State Action Value）
- 状态价值函数：*状态*->输入->状态价值函数->输出->将来预期奖励之和
- 状态行动价值函数：*当前状态和选择行动*->输入->状态行动价值函数->输出->将来预期奖励之和
## Q 函数的更新公式
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$
参数说明：
- Q(s,a): 当前状态 s下执行动作 a 的 Q 值
- α: 学习率，用于控制 Q 值的更新幅度
- R(s, a): 执行动作 a 后得到的即时奖励，*Reward一定是设置的最终奖励，中间状态的奖励就是0*
- γ: 折扣因子，用于权衡当前奖励与未来可能的奖励
- $\max_{a'} Q(s', a')$: 在下一状态 s′ 中所有可能动作的最大 Q 值，所以是一种*off-policy*的学习方法（是直接选择最大的回报，而不是当前策略的期望回报）
主要优势在于使用了*时间差分法*（融合了蒙特卡洛和动态规划）
另：蒙特卡洛法
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ \sum_{i=t}^T R_i - Q(s, a) \right]
$$
蒙特卡洛法需要先生成一些完整的状态、动作、回报值序列，即轨迹，再更新价值函数，这里的$\sum_{i=t}^T R_i$就是当前状态之后的奖励之和
## python例子
从0-5中任意数字为起点，选择+1或-1，最终到达5
```python
import numpy as np

# 定义环境
n_states = 6
actions = [0, 1] # 0: 左, 1: 右
q_table = np.zeros((n_states, len(actions)))

gamma = 0.9 # 折扣因子
alpha = 0.1 # 学习率
epsilon = 0.1 # 探索概率

# Q-Learning 算法
def q_learning(episodes=100):
	for episode in range(episodes):
		state = np.random.randint(0, n_states)
		done = False
			while not done:
				if np.random.uniform(0, 1) < epsilon:
					action = np.random.choice(actions) # 探索
				else:
					action = np.argmax(q_table[state, :]) # 利用
				next_state = min(n_states - 1, state + 1) if action == 1 else max(0, state - 1)
				reward = 1 if next_state == n_states - 1 else 0
				
				q_predict = q_table[state, action]
				q_target = reward + gamma * np.max(q_table[next_state, :])
				q_table[state, action] += alpha * (q_target - q_predict)

				state = next_state
				if state == n_states - 1:
					done = True
q_learning()
print("Q-table after training:")
print(q_table)
```
输出（状态0-5，向左和向右的期望值）
```shell
[[0.05810576, 0.90275731],
 [0.1551038 , 1.41656844],
 [0.37051653, 1.75008744],
 [0.42551719, 2.05367294],
 [0.94811403, 2.34509812],
 [1.52305029, 0.18655817]]
```
# SARSA（State-Action-Reward-State-Action）
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma Q(s', a') - Q(s, a) \right]
$$
与下方的Q-Learning唯一区别在于，Q函数更新公式中的$Q(s'.a')$是通过与$Q(s,a)$一样的Q表（$\epsilon-Greedy$）得到的，而Q-Learning则是直接选择Q值最高的动作
![](https://pics0.baidu.com/feed/79f0f736afc37931fa2ff3f6c007294941a911fd.jpeg@f_auto?token=4f348966aa377196ad1d8f212be65718)
# 对比
*在存在惩罚的环境下，Q-Learning会比SARSA的智能体更趋向于高回报的动作，而忽略其高风险*