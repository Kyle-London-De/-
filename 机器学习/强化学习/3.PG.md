策略梯度方法直接对*策略*进行优化，其目标是最大化累计奖励的期望值
Q-Learning、DQN等算法本质上都是通过对状态动作价值函数（Q函数）的估计，来学习策略，而策略梯度法可以跳过动作价值评估的环节，直接*从状态输入，到策略输出*
![](https://i-blog.csdnimg.cn/blog_migrate/1a293f215493c4afa8335cb43dce8594.png)
策略梯度的核心思想是基于*某种性能参数$J(\theta)$相对于策略参数的梯度*，使用梯度上升的方法*不断调整策略的参数*，以使得策略获得的期望累积奖励最大化，近似于使得$J(\theta)$的梯度上升
# Policy Grandient（PG）
[Policy Grandient 1](https://blog.csdn.net/qq_33302004/article/details/115495686?login=from_csdn)
[Policy Grandient 2](https://blog.csdn.net/qq_41262334/article/details/137771108)
## 过程
在一个episode中，智能体与环境交互产生一个回合的记录序列（Trajectory）
$$
\tau=\{s_1,a_1,s_2,a_2,\dots,s_i,a_i\}
$$
$\tau$ 产生的概率
$$
\begin{aligned}
p_{\theta}\left(\tau\right) & =p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)p(s_3|s_2,a_2)... \\
 & =p(s_1)\prod_{t=1}^Tp_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)
\end{aligned}
$$
$\tau$ 的总回报为
$$
R ( \tau ) = \sum ^T_ { t+1 } r_t 
$$
我们的 $\tau$ 是利用智能体与环境互动产生的，在动作选择过程中存在很多随机性，环境本身也存在很多随机性，所以$R(\tau)$是一个随机变量
所以我们不能用$R(\tau)$评价一个策略网络的好坏，但是我们可以使用$R(\tau)$的期望评价
$$
E [ R ( \tau ) ] = \bar { R } ( \tau ) = \sum _ { \tau } R ( \tau ) p _ { \theta } ( \tau ) = E _ { \tau \sim p _ { \theta } ( \tau ) } [ R ( \tau ) ]
$$
因此，要实现期望最大化，只需要朝着梯度上升的方向调整参数
$$
\begin{aligned}
\nabla \bar { R } ( \tau ) &= \sum _ { \tau } R ( \tau ) \nabla p _ { \theta } ( \tau ) \\
&= \sum _ { \tau } R ( \tau ) p _ { \theta } ( \tau ) \frac { \nabla p _ { \theta } ( \tau ) } { p _ { \theta } ( \tau ) }\\
&= \sum _ { \tau } R ( \tau ) p _ { \theta } ( \tau ) \nabla \log p _ { \theta } ( \tau )\\
&= E _ { \tau \sim p _ { \theta } ( \tau ) } \left[ R ( \tau ) \nabla \log p _ { \theta } ( \tau ) \right]\\
&\approx \frac { 1 } { N } \sum _ { n = 1 } ^ { N } R ( \tau ) \nabla \log p _ { \theta } ( \tau )\\
&= \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } R \left( \tau ^ { n } \right) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
\end{aligned}
$$
直观理解这个梯度公式：*我们知道在$s_t$状态下执行了$a_t$动作，如果最终的回报$R(\tau^n)$是好的，那么就增加$(s_t|a_t)$的选择概率，否则减少*
训练过程：
- 先初始化一个策略网络
- 用这个网络进行N次游戏，产生N个$\tau$（游戏记录）
- 利用N个$\tau$进行梯度上升，调整策略网络的参数
$$
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } R \left( \tau ^ { n } \right) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
$$
$$
\theta\leftarrow\theta+\eta\nabla\bar{R}(\tau)
$$
- 重复2、3
## Tips
- 添加基线（baseline）：许多问题中回报是没有负值的，并且采样也是有限的，所以只要采样到都可以提升之后选中的概率，而其他没有选中的则相当于概率一直在下降。设置基线可以一定程度上修正这一问题，汇报大于基线则概率上升，反之下降，最简单的基线选择可以用本次采样的平均回报
$$
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } (R \left( \tau ^ { n } \right)-b) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
$$
- 合理分配信用（credit）：在一个episode中，并不是所有的动作都具有相同的调整权重，但最终的结果好坏，并不代表过程中每一个决策的好坏。当前的决策并不会对之前的决策回报产生影响，许久之前的决策对当前的决策回报产生的影响也很小，可以引入衰减因子（discount factor $\gamma$）
$$
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } (\sum^T_{t'=t}r^n_{t'}\gamma^{t'-t} -b) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)\quad,\quad\gamma\in[0,1]
$$
定义优势函数（advantage function）$A^\theta(s_t,a_t)$，即上式的$(R-b)$项
$$
A^\theta(s_t,a_t)=\sum^T_{t'=t}r^n_{t'}\gamma^{t'-t} -b
$$
表示$s_t$状态下采取$a_t$动作的优势程度，通常可以由一个网络估计得到，如A3C中的critic网络
