[AC、A2C、A3C 1](https://zhuanlan.zhihu.com/p/62100741/)
[AC、A2C、A3C 2](https://cloud.tencent.com/developer/article/1375650)
# Actor Critic框架
在策略梯度中，使用下式对策略参数的调整幅度进行评价
$$
\begin{aligned}
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } R \left( \tau ^ { n } \right) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
\end{aligned}
$$
其中， $p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)$ 为*Actor网络*（多用神经网络表示）
$\sum _ { t = 1 } ^ { T } R \left( \tau ^ { n } \right)$ 为*Critic网络*的其中一种表达形式，其他的Critic网络如
- $\sum _ { t = 1 } ^ { T } R \left( \tau ^ { n } \right)$ 使用轨迹的总回报
- $\sum _ { t = t^{'} } ^ { T _ { n } } R \left( \tau ^ { n } \right)$ 使用动作后的回报
- $A^\theta(s_t,a_t)=\sum^T_{t'=t}r^n_{t'}\gamma^{t'-t} -b$ 使用加入基线的形式
- $Q^\theta(s_t,a_t)$ 使用状态行动价值函数
- $A^\theta(s_t,a_t)$ 使用优势函数
- $R(\tau)+V^\theta(s_{t+1})-V^\theta(s_t)$ 使用TD-error
前三种直接使用轨迹的回报累计回报，策略梯度计算*不存在偏差*，但由于需要积累多步的回报，*方差较大*；后三种利用动作价值函数等方法来代替误差积累，优点在于*方差较小*，但由于用到了*逼近*的方法，策略梯度*存在偏差*，后三者为经典的AC算法
比如使用Q函数的AC法，更新过程如下：
先通过类似于
$$
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } Q^{\pi_{\theta}} \left( s_t ^ { n } ,a_t^n\right) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
$$
更新Actor网络的参数（梯度）
再通过类似于
$$
loss=\frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } }(r^n_t + \max_{a^n_{t+1}} Q^{\pi_\theta}(s^n_{t+1}, a^n_{t+1}) - Q(s^n_{t}, a^n_{t}) )^2
$$
更新Q值也就是R函数
# Advantage Actor Critic（A2C）
为AC法的Q函数增加一个基线，使得反馈有正有负，这里的基线常用*状态价值函数*表示，即
$$
Q^{\pi}(s^n_{t}, a^n_{t}) - V(s^n_{t}) 
$$
$$
V(s^n_{t})=\sum p_\theta ( a _ { t } ^ { n } \mid s _ { t } ^ { n })Q^{\pi}(s^n_{t}, a^n_{t})
$$
但我们一次只能更新一个网络，所以转换为
$$
r^n_t+V^{\pi}(s^n_{t+1}) - V(s^n_{t}) 
$$
虽然会增加一定方差，但一定程度上可以忽略
因此，Actor网络梯度更新修改为
$$
\nabla \bar { R } ( \tau ) = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } } (Q^{\pi_{\theta}} \left( s_t ^ { n } ,a_t^n\right)-V^{\pi_{\theta}} \left( s_t ^ { n }\right)) \nabla \log p _ { \theta } \left( a _ { t } ^ { n } \mid s _ { t } ^ { n } \right)
$$
Critic网络变为估计状态价值函数V的网络
$$
loss=\frac { 1 } { N } \sum _ { n = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { n } }(r^n_t+V^{\pi}(s^n_{t+1}) - V(s^n_{t}) )^2
$$
# A3C（Asynchronous Advantage Actor Critic）
神经网络训练时，需要数据是独立同分布的，为了打破数据之间的相关性，DQN和DDPG都采用了经验回放的方式，但还有另一种方法，*异步*
A3C模型中，存在多个A2C模型，从全局网络中获取参数，再独自训练，最后使用多个A2C模型数据进行全局网络的更新