# 深度Q网络（DQN）
[DQN](https://zhuanlan.zhihu.com/p/362076700)
![](https://i-blog.csdnimg.cn/blog_migrate/be3927ab145b245737cb38af3cbccdf5.png)
在Q-Learning的基础上，引入神经网络来逼近 Q 函数，即
$$
Q(s,a;\theta)
$$
## 关键技术
- 经验回放（Experience Replay）：通过存储智能体与环境交互的经验，随机抽取小批量样本进行训练，减少数据之间的相关性
- 目标网络（Target Network）：通过引入一个目标网络来稳定训练，目标网络的参数每隔一段时间才更新一次（硬更新，直接复制）
## 流程
![](https://pic4.zhimg.com/v2-a9d5515c47953893e4810bc218b75129_1440w.jpg)
- 产生经验池，若干$\{s_t,a_2,s_{t+1},r_{t+1}\}$
- 以Q网络的输出$Q(s_t,a_t)$作为*状态$s_t$下动作$a_t$的预测价值估计*
- 以Target Q网络的输出$Q(s_{t+1},a_{t+1})$（*采用完全贪婪，直接选择最大Q值*）与奖励得到的$r_{t+1}+\gamma Q(s_{t+1},a_{t+1})$作为*状态$s_t$下动作$a_t$的真实价值估计*
- 通过误差进行反向传播，loss函数可以选择方差等
- 一定周期内使用Q网络更新Target Q网络
# DDQN（Double Deep Q Network）
DQN算法并不能保证一直收敛，这种估计目标价值的算法，过于乐观地高估了一些情况下的行为价值，导致算法会*将次优行为价值一致认为是最优行为价值*，导致最终不能收敛到最佳价值函数，而DDQN较好的解决了这一问题
DDQN与DQN唯一不同在于，在$Q(s_{t+1},a_{t+1})$的选择上，先由Q网络选择出Q值最大的动作，再在Target Q网络中找到这个动作对于的Q值作为$Q(s_{t+1},a_{t+1})$
# DDPG（深度确定性策略梯度）
深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）是一种适用于连续动作空间的深度强化学习算法，结合了 DQN 和策略梯度方法的优点
DDPG 使用两个网络：
- *Actor 网络*：用于选择动作
- *Critic 网络*：用于评估 Actor 的策略
DDPG 还使用了*目标网络和经验回放机制*，以稳定训练过程
## 网络结构
![](https://pic2.zhimg.com/v2-af2244e2f9d47596624b8776e00e8b2b_1440w.jpg)
一个Actor网络，一个Target Actor网络，输入$s_t$输出$a_t$；一个Critic网络，一个Target Critic网络，输入$s_t,a_t$输出$Q(s_t,a_t)$
## 参数更新
- 产生若干经验$\{s_t,a_t,s_{t+1},r_{t+1}\}$，特别的，$a_t$是Actor网络输出的动作加了*一定噪声*以后得到的
![](https://picx.zhimg.com/v2-faad96926ae81a8dcb2e8dd869924953_1440w.jpg)
- 更新Actor网络，将经验的$s_t$输入到Actor网络中得到$a_{t\_predict}$（不加噪声），将$s_t,a_{t\_predict}$输入到Critic网络中，得到Q值，以-Q作为loss修正Actor网络（-Q越小越好）
![](https://pica.zhimg.com/v2-3231907a8274c7679a0d887fda096b32_1440w.jpg)
- 更新Critic网络，使用Critic网络输出的$Q(s_t,a_t)$和Target Critic网络输出处理得到的$r_{t+1}+\gamma Q(s_{t+1},a_{t+1})$进行Critic网络修正
![](https://pica.zhimg.com/v2-2411c65e3c76efe1e3bc9a28e07ee5e2_1440w.jpg)
- 一定周期内更新Target Actor网络和Target Critic网络（软更新）