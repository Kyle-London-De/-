# 柔性动作-评价（SAC）
[SAC算法 1](https://zhuanlan.zhihu.com/p/385658411)
[SAC算法 2](https://cloud.tencent.com/developer/article/2485367)
SAC算法解决的问题是*离散动作空间和连续动作空间*的强化学习问题，off-policy算法
## 优势
- 最大熵强化学习：在最大化累计奖励的同时，通过熵最大化策略的随机性，鼓励探索
- 双Q网络：缓解Q值过估计的问题
- 目标网络：使用目标网络稳定Q值计算
## 网络结构
SAC算法有两种，一种是一个Actor网络，四个Q Critic网络；一种是*一个Actor网络，两个V Critic网络（一个V Critic网络，一个Target V Critic网络），两个Q Critic网络*，以后者展开
![](https://pic4.zhimg.com/v2-47e90c382024d6664d73cc14b5755ad3_1440w.jpg)
Actor网络输入为状态，输出为动作概率（离散空间）或动作概率分布（连续空间）$p_{\theta}(a_t\mid s_t)$
Critic网络输入为状态，输出V Critic网络为状态价值$v(s_t)$，Q Critic网络为动作（-状态对）价值$q_i(s_t,a_i)$
## 熵
SAC算法中为了鼓励探索，引入了熵的概念，所以对网络的训练不同于常规的TD3、PPO等算法，其训练目标不同
在SAC算法中，Actor网络输出动作的好坏，取决于一个综合的指标，包含动作价值$q_i(s_t,a_i)$和熵$h$
## 经验池
![](https://pic2.zhimg.com/v2-1a103fddab650aa839ec8c552c2641ef_1440w.jpg)
已知一个状态$s_t$，输入到Actor网络中，输出动作概率，依据动作概率选择动作$a_2$，将$a_2$输入到环境中，得到$s_{t+1}$和$r_{t+1}$，如是得到一个经验$\{s_t,a_2,s_{t+1},r_{t+1}\}$，将其放入到经验池中
经验池存在的意义就是为了消除各个经验之间的相关性，因为强化学习中前后动作通常是强相关的，而将他们打散放入经验池中，再通过经验池训练，可以训练出更好的神经网络
## 参数更新
### Q Critic网络更新
![](https://pica.zhimg.com/v2-c186876988be41a10eb85923455a54c6_1440w.jpg)
从经验池中拿取经验，如$\{s_t,a_2,s_{t+1},r_{t+1}\}$进行Q Critic网络更新，基于最优贝尔曼方程
- 通过Target V Critic网络计算，取$U_t^{(q)}=r_t+\gamma V(s_{t+1})$为*状态$s_t$的真实价值估计*
- 通过Q Critic网络，取对应动作$a_2$的动作价值$q_i(s_t,a_2)$为*状态$s_t$的预测价值估计*，$i$代表不同Q Critic网络
- 通过MSELoss修正，更新两个Q Critic网络的参数
其中MSELoss是使用一个batch的数据，以误差的平⽅和作为距离（欧氏距离）更新参数
$$
loss=\frac{1}{B}\sum_{(s_t,a_t,s_{t+1},r_{t+1})\in B} \left[ q_i(s_t,a_t;\omega^{(i)})-U_t^{(q)} \right]^2
$$
### V Critic网络更新
![](https://pic2.zhimg.com/v2-a11e154e91447c9a0c290a10a1ec8545_1440w.jpg)
从经验池中拿取经验，如$\{s_t,a_2,s_{t+1},r_{t+1}\}$进行V Critic网络更新
- 使用含熵的式子进行*状态$s_t$的真实价值估计*
$$
U _ { t } ^ { ( v ) } = E _ { a _ { t } ^ { \prime } \sim \pi \left( \cdot \mid s _ { t } ; \theta \right) } \left[ \min _ { i = 0 , 1 } q _ { i } \left( s _ { t } , a _ { t } ^ { \prime } ; w ^ { ( i ) } \right) - \alpha \ln \pi \left( a _ { t } ^ { \prime } \mid s _ { t } ; \theta \right) \right]

$$
其中
1. $\min _ { i = 0 , 1 } q _ { i } \left( s _ { t } , a _ { t } ^ { \prime } ; w ^ { ( i ) } \right)$为两个Q Critic网络输出的较小者
2. $- \alpha \ln \pi \left( a _ { t } ^ { \prime } \mid s _ { t } ; \theta \right)$则是利用$s_t$状态下进行$a_2$的动作概率求取熵，概率越接近1，熵越小，越稳定，越接近真实值（参考$-ln(x),x\in(0,1)$）
3. $\alpha$是熵的奖励系数，用于权衡熵与q值的比重
4. $E _ { a _ { t } ^ { \prime } \sim \pi \left( \cdot \mid s _ { t } ; \theta \right) }[\dots]$指的是对Actor网络输入状态$s_t$得到的*所有动作*对应的期望

- 使用V Critic网络的输出作为*状态$s_t$的预测价值估计*
- 同样通过MSELoss修正
$$
loss=\frac{1}{B}\sum_{(s_t,a_t,s_{t+1},r_{t+1})\in B} \left[ v(s_t;\omega^{(v)})-U_t^{(v)} \right]^2
$$
### Actor网络更新
![](https://pic3.zhimg.com/v2-499c999de7e71ed033bc7cf878114c7e_1440w.jpg)
$$
loss=-\frac{1}{B}\sum_{(s_t,a_t,s_{t+1},r_{t+1})\in B} E _ { a _ { t } ^ { \prime } \sim \pi \left( \cdot \mid s _ { t } ; \theta \right) } \left[ q _ { i } \left( s _ { t } , a _ { t } ^ { \prime } ; w ^ { ( i ) } \right) - \alpha \ln \pi \left( a _ { t } ^ { \prime } \mid s _ { t } ; \theta \right) \right]
$$
其中，q值由任一个Q Critic网络即可（理论上俩Q Critic网络功能等价）
对于离散空间，期望可以转化为求和
### Target V Critic网络更新
采用软更新规则，从V Critic网络参数中更新
$$
\theta_{target}=\tau\theta+(1-\tau)\theta_{target},\tau \in (0,1)
$$
区别与硬更新直接复制V Critic网络参数，软更新是渐进式的更新Target V Critic网络的参数