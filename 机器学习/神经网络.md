[参考](https://blog.csdn.net/qq_59702185/article/details/143501764)
# 简介
人工智能>机器学习>深度学习
深度学习与一般机器学习的主要区别在于是否需要*人工设计特征并进行特征提取*
常见神经网络：
1. 卷积神经网络（CNN）
2. 循环神经网络（RNN）
3. 生成对抗网络（GAN）
4. 深度强化网络（DRN）
优点：
1. 精度高，性能优于其他机器学习
2. 可以近似任意非线性函数
3. 有大量库和框架可调用
缺点：
1. 无法建模
2. 训练成本高
3. 网络结构复杂，需要调整超参数
4. 需要大量数据集，数据量不够容易过拟合
# 神经元
![](https://i-blog.csdnimg.cn/direct/eb6153f87ec44530b96542d03cd9a22e.png)
神经元由输入x，权重w，偏置b和输出y组成，其中输出为$f \left(\sum_iw_ix_i+b\right)$，f为激活函数
网络中没有循环，只向着一个方向移动
网络包括：
1. 输入层
2. 输出层
3. 隐藏层
网络特点：
1. 同一层神经元之间没有连接
2. 前一层神经元与后一层*所有*神经元相连，前一层神经元的输出就是后一层神经元的输入
# 激活函数
## sigmoid函数
$$f(x)=\frac{1}{1+e^{-x}}$$
![](https://i-blog.csdnimg.cn/direct/31860898b2fd492e9be247f79fd42adc.png)
定义域内处处可导，且两侧导数逐渐趋近于0。 如果输入的值很⼤或者很⼩的时候，那么函数的梯度（函数的斜率）会⾮常⼩，在反向传播的过程中，导致了向低层传递的梯度也变得⾮常⼩。此时，⽹络参数很难得到有效训练。这种现象被称为梯度消失。 ⼀般来说， sigmoid ⽹络在 5层之内就会产⽣*梯度消失现象*。⽽且，该激活函数并不是以0为中⼼的，所以在实践中这种激活函数使⽤的很少。sigmoid函数⼀般只⽤于*⼆分类的输出层*。因为他是在（0,1）之间的。
## tanh函数（双曲正切曲线）
$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
![](https://i-blog.csdnimg.cn/direct/fa2e1e627f18439b923bcb027adb5a6c.png)
tanh也是⼀种⾮常常⻅的激活函数，与sigmoid相⽐，它是*以0为中⼼的*，使得其收敛速度要⽐sigmoid快，减少迭代次数。然⽽，tanh两侧的导数也为0，同样会造成*梯度消失*。若使⽤时可在*隐藏层使⽤*tanh函数，在输出层使⽤sigmoid函数。
## ReLU函数
$$f(x)=\max(0,x)$$
![](https://i-blog.csdnimg.cn/direct/b95db4f2899943eea122e057965d0fe2.png)
ReLU是⽬前*最常⽤的*激活函数。 从图中可以看到，当x<0时，relu导数为0，⽽当x>0时，则不存在饱和问题。所以，ReLU 能够*在x>0时保持梯度不衰减*，从⽽缓解梯度消失问题。然⽽，随着训练的推进，部分输⼊会落⼊⼩于0区域，导致对应权重⽆法更新。这种现象被称为“*神经元死亡*”。采⽤sigmoid函数，计算量⼤（指数运算），反向传播求误差梯度时，求导涉及除法，计算量相对⼤，⽽采⽤ReLU激活函数，整个过程的*计算量节省很多*。sigmoid函数反向传播时，很容易就会出现梯度消失的情况，从⽽⽆法完成深层⽹络的训练。ReLU会使⼀部分神经元的输出为0，这样就造成了⽹络的*稀疏性*，并且减少了参数的相互依存关系，缓解了过拟合问题的发⽣。
## Leaky_ReLU函数
$$f(x)=\max(0.1x,x)$$
![](https://i-blog.csdnimg.cn/direct/770aa9fbeada4f8eb43805b201972988.png)
leaky_relu解决了当x<0时，神经元消失的情况
## softmax函数
$$softmax(x_i)=\frac{e^{z_i}}{\sum_je^{z_j}}$$
![](https://i-blog.csdnimg.cn/direct/3c930e1d26a54e9d93a1b04ba9217b17.png)
softmax⽤于*多分类*过程中，它是⼆分类函数sigmoid在多分类上的推⼴，⽬的是将多分类的结果以概率的形式展现出来，softmax直⽩来说就是将⽹络输出的logits通过softmax函数，就映射成为(0,1)的值，⽽这些值的累和为1（满⾜概率的性质），那么我们将它理解成概率，选取概率最⼤（也就是值对应最⼤的）接点，作为我们的预测⽬标类别。
## 激活函数的选择
1. 隐藏层：*优先选ReLu，注意神经元死亡的问题，效果不好尝试leaky_ReLU，不要使用sigmoid，可以尝试tanh*
2. 输出层：*二分类用sigmoid，多分类用softmax，回归问题用identity*
# 参数初始化
在深度学习中，神经网络的权重初始化方法对模型的收敛速度和性能有着至关重要的影响，神经网络其实就是*对权重参数w的不停迭代更新，以达到更好的性能*，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸的问题，但是对于处理这两个问题是有很大的帮助的，并且十分有利于模型性能和收敛速度
对于某⼀个神经元来说，需要初始化的参数有两类：⼀类是权重w，还有⼀类是偏置b，偏置b初始化为0即可，⽽权重w的初始化⽐较重要
## 随机初始化
随机初始化从*均值为0，标准差为1的高斯分布*中取样，使用一些很小的值对参数w进行初始化
## 标准初始化
权重参数初始化从区间均匀随机取值，即在（-1/√d，1/√d）*均匀分布*中生成当前神经元的权重，其中d为每个神经元的输入数量
## Xavier初始化
网络训练的过程中, 容易出现*梯度消失*(梯度特别的接近0)和*梯度爆炸*(梯度特别的大)的情况，导致大部分反向传播得到的梯度不起作用或者起反作用
研究人员希望能够有一种好的权重初始化方法，让网络前向传播或者反向传播的时候，卷积的输出和前传的*梯度比较稳定*，合理的方差既保证了数值一定的*不同*，又保证了数值一定的*稳定*(通过卷积权重的合理初始化, 让计算过程中的数值分布稳定)
Xavier初始化也称为Glorot初始化，因为发明人为Xavier Glorot，Xavier initialization是 Glorot 等人为了解决随机初始化的问题提出来的另一种初始化方法，他们的思想就是*尽可能的让输入和输出服从相同的分布*，这样就能够*避免后面层的激活函数的输出值趋向于0*
因为权重多使用*高斯或均匀分布*初始化，而两者不会有太大区别，只要*保证两者的方差一样*就可以了
### 正态化Xavier初始化
以0为中心，标准差为$stddev = \sqrt{2 / (fan_{in} + fan_{out}})$的*正态分布*中抽取样本，其中$fan_{in}$是输入神经元的个数，$fan_{out}$是输出神经元的个数
### 标准化Xavier初始化
从$[-limit, limit]$中的*均匀分布*中抽取样本，其中limit是$\sqrt{6 / (fan_{in} + fan_{out}）}$，其中$fan_{in}$是输⼊神经元的个数，$fan_{out}$是输出的神经元个数
## He初始化
He初始化，也称为Kaiming初始化，出⾃何恺明之⼿，它的基本思想是正向传播时，*激活值的⽅差保持不变*；反向传播时，关于*状态值的梯度的⽅差保持不变*
### 正态化He初始化
以0为中心，标准差为$stddev = \sqrt{2 / fan_{in}}$的截断*正态分布*中抽取样本，其中$fan_{in}$是输入神经元的个数
### 标准化He初始化
从$[-limit, limit]$中的*均匀分布*中抽取样本，其中limit是$\sqrt{6 / fan_{in}}$，其中$fan_{in}$是输⼊神经元的个数
# 损失函数
在深度学习中，损失函数是用来*衡量模型参数质量*的函数，衡量的方式是*比较网络输出与真实值输出的差异*
## 分类任务损失函数
### 多分类任务（交叉熵损失）
在多分类任务通常*使⽤softmax将logits转换为概率*的形式，所以多分类的交叉熵损失也叫做softmax损失
$$
L = -\sum^{n}_{i=1}y_i\log(S(f_\theta(x_i)))
$$
其中，y是样本x属于某⼀个类别的真实概率，⽽f(x)是样本属于某⼀类别的预测分数，S是softmax函数，所以L是⽤来衡量p、q之间差异性的损失结果
### 二分类任务（交叉熵损失）
在处理⼆分类任务时，我们不再使⽤softmax激活函数，⽽是*使用sigmoid激活函数*，损失函数也相应的进⾏调整，使⽤⼆分类的交叉熵损失函数
$$
L=-y\log\hat{y}-(1-y)\log(1-\hat{y})
$$
其中，y是样本x属于某⼀个类别的真实概率，⽽$\hat{y}$是样本属于某⼀类别的预测概率，所以L⽤来衡量真实值与预测值之间差异性的损失结果
## 回归任务损失函数
### MAE损失函数
Mean absolute loss（MAE）也被称为L1 Loss，是以*绝对误差*作为距离
$$
L=\frac{1}{n}\sum^{n}_{i=1}|y_i-f_\theta(x_I)|
$$
其中，$y_i$为真实值，$f_\theta(x_i)$为预测值
![](https://i-blog.csdnimg.cn/direct/09728b9fd39040dc9477c42192e5a024.png)
由于L1 loss具有*稀疏性*，为了惩罚较⼤的值，因此常常将其作为*正则项*添加到其他loss中作为约束
L1 loss的最⼤问题是梯度*在零点不平滑，会导致跳过极⼩值*
### MSE损失函数
Mean Squared Loss/ Quadratic Loss（MSE loss）也被称为L2 loss，或欧⽒距离，它以误差的平⽅和作为距离
$$
L=\frac{1}{n}\sum^{n}_{i=1}(y_i-f_\theta(x_i))^2
$$
其中，$y_i$为真实值，$f_\theta(x_i)$为预测值
![](https://i-blog.csdnimg.cn/direct/a8780bbe5f8c40799f584994d9cfe9c1.png)
L2 loss也常常作为*正则项*，*当预测值与⽬标值相差很⼤时, 梯度容易爆炸*
### smooth L1损失函数
$$
smooth_{L_1}(x)=
\begin{cases}
0.5x^2\quad & if|x|<1\\
|x|-0.5 & otherwise
\end{cases}
$$
该函数实际上就是⼀个分段函数，在$[-1,1]$之间实际上就是L2损失，这样解决了L1的不光滑问题，在$[-1,1]$区间外，实际上就是L1损失，这样就解决了离群点梯度爆炸的问题。通常在*⽬标检测*中使⽤该损失函数 
### 正则项的作用
正则项的作用机制主要是通过*惩罚复杂的模型来引导模型学习简单的特征*
1. L1正则化通过惩罚权重的绝对值来促使模型学习*稀疏的权重*，即许多权重为0
2. L2正则化通过惩罚大的权重来促使模型学习*较小的权重*，从而使模型更加平滑
3. 而Dropout通过随机丢弃神经元来*减少模型对特定特征的依赖*，从而提高模型的泛化能力‌
> Dropout是一种在训练过程中随机丢弃网络中一部分神经元的方法，*每个神经元一定的概率（一般0.5）被暂时从网络中移除*，故整个神经网络的计算过程中，不会特别依赖于哪个特征值，这就保证了每个神经元的权重都不会太大，从而减少模型的复杂度
# 优化算法
## 梯度下降法
从数学上的⻆度来看，*梯度的⽅向是函数增⻓速度最快的⽅向，梯度的反⽅向就是函数减少最快的⽅向*