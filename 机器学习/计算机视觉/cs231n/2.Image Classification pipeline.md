# K-近邻算法
## Nearest Neighbor 近邻算法
1. train：记录数据和标签，O(1)
2. predict：为新的数据寻找最接近的训练数据，输出对应标签，O(n)
L1距离（曼哈顿距离）：计算两张图片对应像素之差的和，二维距离是正菱形（旋转坐标系会改变L1距离，适合*各向异性*）
$$d_1(I_1,I_2)=\sum_{p}|I_1^p-I_2^p|$$
预测时长反而比训练时长长，这并不符合应用场景
## K-Nearest Neighbors K-近邻算法
找到K个最近的数据，在一定范围内随着K提升，噪声更少，边缘更平滑，*K是一个超参数*
L2距离（欧式距离）：计算对应像素之差的平方和，再开更号，二维距离是圆形（旋转坐标系不会改变L2距离，适合*各向同性*）
$$d_1(I_1,I_2)=\sqrt{\sum_{p}|I_1^p-I_2^p|}$$
## 超参数选择
1. 将数据集分为train、validation、test三个数据集，分别用于训练、验证和测试，验证集的使用是在*训练中*的，用于优化模型（选择参数），测试集的使用则是在*训练后*的，用于呈现模型的好坏
2. 交叉验证，使用fold折叠数据，将数据集分为训练集和测试集，训练集再分为多个fold，在训练过程中选择性的*隐藏某些fold，循环训练*，适用于小规模训练（大规模循环训练会消耗大量资源）
- K-近邻算法并不适用于计算机图像技术，首先预测时间长，其次逐像素的比较并不是我们期望的图像相似的判断，并且*高维度的数据很难找到覆盖所有空间的样本数据*
# 线性分类器 Linear Classification
图像处理技术的基础组成部分之一
$$
f(x,W)=Wx+b
$$
输入图像和参数的权重，输出各个类型的可能性（0-1）
相比于K-近邻算法，线性分类器不再保存整个图像作为样本，而是*对特征进行概括，形成权重w*，使算法更加高效
比如32x32x3的图像，转化为列向量3072x1，则权重为10x3072（10个类别），函数最终输出10个数字
1. 将权重视为各个分类的模板还原为图像，可以一定程度上看到模型*在当前类别寻找的特征*
2. 从高维观察，线性分类器用*高维平面*划分各个类别
3. 线性分类器难以应对*多模态*情况，比如样本空间中存在*多个非连续空间*是一个类别的情况