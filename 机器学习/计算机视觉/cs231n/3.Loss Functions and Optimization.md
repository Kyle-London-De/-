# Loss Functions 损失函数
$L_i()$ 函数表示预测的结果与真实值之间的差距，N代表图像数
$$
L=\frac{1}{N}\sum_N L_i(f(x_i,W),y_i)
$$
## Multiclass SVM Loss（Hinge Loss）
1. 样本：$(x_i,y_i)$，图像和类别标签
2. 预测：$s=f(x_i,W)$，得到预测的向量。即每个类型的可能性
3. 计算SVM损失：$L_i=\sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)$ 又称*铰链损失函数*（Hinge Loss），计算正确类别的分数与其他类别分数的差并加上安全系数，与0取最大值，求和作为损失，这里*安全系数1的取值并不重要*，在W的学习中会*像比例尺一样被淡化*，就像损失的实际大小并不重要
4. 计算整个训练数据的损失：$L=\frac{1}{N}\sum_{i=1}^N L_i$，如果W初始化为0或者其他相近的随机数，则第一次的损失应该接近1，可用于代码检查
5. 如果将铰链损失函数中的最大差修改为最大平方差，这就不再是线性损失函数了，一般来说，非平方的版本更加标准，但平方铰链损失函数也有适用的地方
6. 满足损失为0的W权重并不是唯一的，比如某个W的整倍数
## Regularization 正则化（范数）
$$
L(W)=\frac{1}{N}\sum^N_{i=1} L_i(f(x_i,W),y_i)+\lambda R(W)
$$
$R(W)$是一个正则化损失，也称为Occam‘s Razor，在训练中*督促模型进行简单化*（例如更倾向于用低阶表达式拟合曲线），更符合实际应用，$\lambda$是一个超参数用于平衡两个损失
1. L1 Regularization：$R(W)=\sum_k \sum_l |W_{k,l}|$
2. L1 Regularization：$R(W)=\sum_k \sum_l W_{k,l}^2$
3. Elastic net(L1+L2)：$R(W)=\sum_k \sum_l \beta W_{k,l}^2+|W_{k,l}|$
4. Max norm regularization
5. Dropout
6. Fancier
## Softmax Classifier（Multinomial Logistic Regression，多项式逻辑递归）
各个类别的概率，*先指数化，再归一化*，使所有概率之和为1
$$
P(Y=k|X=x_i)=\frac{e^{s_k}}{\sum_j e^{s_j}}\quad,\quad s=f(x_i;W)
$$
## Cross-entropy Loss 交叉熵损失
损失函数，利用-log函数（0，1）范围内的单调性
$$
L_i=-logP(Y=y_i|X=x_i)
$$
Softmax的损失函数的最小值0和最大值无穷大，在实际应用中都无法达到，一部分原因是计算机底层运算的限制
## 比较
*SVM 只是拟合权重 W，而 Softmax 则是希望不断提高模型的质量*（提高正确类别的概率）
# Optimization 优化
1. Random search：随机搜索+线性分类器，效率很低，最新的技术也有达到95%
2. Follow the slope：根据梯度优化W，向梯度减小的方向优化
## gradient 梯度
$$
\frac{df(x)}{dx}=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}
$$
- 数值梯度：有限差分计算Loss函数的梯度dW，为某个权重加上一个很小的变化量，作为步长，求损失的相对变化（*步长是一个超参数，也叫学习率，在训练最初确定，优先于模型大小正则化参数等*）
- 解析梯度：在更复杂模型中，这样计算不现实，一般利用微分思想，得出*梯度的函数*，直接计算梯度，也存在步长，即为每次修正模型的程度
## Stochastic Gradient Descent（随机梯度下降法，SGD）
$$
L(W)=\frac{1}{N}\sum^N_{i=1} L_i(f(x_i,W),y_i)+\lambda R(W)
$$
$$
\nabla_WL(W)=\frac{1}{N}\sum^N_{i=1} \nabla_WL_i(f(x_i,W),y_i)+\lambda \nabla_WR(W)
$$
对整个数据集求梯度是很慢的，一般使用 batch *小批量*，取2的整数倍的样本进行梯度递归
# Image Features
1. 提取图像特征权重，整合为目标权重
2. 特征转换可以将多模态问题转化为线性分类器可以处理的问题（比如从直角坐标系转换为极坐标系）
## 例子
1. Color Histogram：彩色直方图，不同的权重赋予不同的颜色和深度
2. Histogram of Oriented Gradient（HoG）：梯度方向直方图，将图像均匀分为小块，计算每个小块的梯度方向，受人类视觉启发，以此寻找边缘信息
3. Bag of Words：词袋，图像拆分成多个小块（颜色块、边缘快等），受单词和字母关系的启发，统计这些小块的数量来区分类别