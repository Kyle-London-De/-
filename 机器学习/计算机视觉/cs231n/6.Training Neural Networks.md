# Part 1
## Activation Functions 激活函数
### sigmoid函数
$$f(x)=\frac{1}{1+e^{-x}}$$
![](https://i-blog.csdnimg.cn/direct/31860898b2fd492e9be247f79fd42adc.png)
1. 梯度消失问题：输入过大或过小，函数的梯度都会消失（斜率为0），梯度的反向传播失效
2. 输出非0中心：输出并不是以0为中心的，无论输入如何，所有输出都是正的，在 $f(\sum_i w_ix_i+b)$ 中，权重w的梯度$\frac{\partial f}{\partial w}$就是输入x，如果x所有值都为正，则w的梯度也恒正，导致权重的更新总是增加或者总是减小，那么期望权重相较于初始权重正负都有的情况就无法实现优化
3. 指数计算昂贵
定义域内处处可导，且两侧导数逐渐趋近于0。 如果输入的值很⼤或者很⼩的时候，那么函数的梯度（函数的斜率）会⾮常⼩，在反向传播的过程中，导致了向低层传递的梯度也变得⾮常⼩。此时，⽹络参数很难得到有效训练。这种现象被称为梯度消失。 ⼀般来说， sigmoid ⽹络在 5层之内就会产⽣*梯度消失现象*。⽽且，该激活函数并不是以0为中⼼的，所以在实践中这种激活函数使⽤的很少。sigmoid函数⼀般只⽤于*⼆分类的输出层*。因为他是在（0,1）之间的。
### tanh函数（双曲正切曲线）
$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
![](https://i-blog.csdnimg.cn/direct/fa2e1e627f18439b923bcb027adb5a6c.png)
tanh也是⼀种⾮常常⻅的激活函数，与sigmoid相⽐，它是*以0为中⼼的*，使得其收敛速度要⽐sigmoid快，减少迭代次数。然⽽，tanh两侧的导数也为0，同样会造成*梯度消失*。使⽤时可在*隐藏层使⽤*tanh函数，在输出层使⽤sigmoid函数。
### ReLU函数
$$f(x)=\max(0,x)$$
![](https://i-blog.csdnimg.cn/direct/b95db4f2899943eea122e057965d0fe2.png)
1. 输出非0中心
2. 神经元死亡：小于0的区域，没有输出，会导致如果某些数据经过神经元输出为0，则这些数据不会得到反向梯度，该神经元也就不会更新，神经元死亡（10%-20%）
ReLU是⽬前*最常⽤的*激活函数。 从图中可以看到，当x<0时，relu导数为0，⽽当x>0时，则不存在饱和问题。所以，ReLU 能够*在x>0时保持梯度不衰减*，从⽽缓解梯度消失问题。然⽽，随着训练的推进，部分输⼊会落⼊⼩于0区域，导致对应权重⽆法更新。这种现象被称为“*神经元死亡*”。采⽤sigmoid函数，计算量⼤（指数运算），反向传播求误差梯度时，求导涉及除法，计算量相对⼤，⽽采⽤ReLU激活函数，整个过程的*计算量节省很多*。sigmoid函数反向传播时，很容易就会出现梯度消失的情况，从⽽⽆法完成深层⽹络的训练。ReLU会使⼀部分神经元的输出为0，这样就造成了⽹络的*稀疏性*，并且减少了参数的相互依存关系，缓解了过拟合问题的发⽣。
### Leaky_ReLU函数
$$f(x)=\max(0.1x,x)$$
![](https://i-blog.csdnimg.cn/direct/770aa9fbeada4f8eb43805b201972988.png)
1. 没有梯度饱和的情况
2. 计算高效
3. 收敛比 sigmoid/tanh 快
4. 神经元不会死亡
leaky_relu解决了当x<0时，神经元消失的情况
### PReLU
$$
f(x)=\max(ax,x)
$$
将a作为一个参数在训练中更新，而不是一个超参数
### ELU
$$
f(x)=\begin{cases} x & \text{if }x>0 \\ \alpha(e^x-1) & \text{if }x\leq0 \end{cases}
$$
![](https://i-blog.csdnimg.cn/blog_migrate/615737c905db3dc2eaff333c9abf0197.png)
1. 拥有ReLU的所有优点
2. 均值接近0的输出
3. 在输入为负的部分存在梯度饱和，对噪声数据具有鲁棒性
4. 计算需要指数
### Maxout ”Neuron“
$$
\max(w_1^Tx+b_1,w_2^Tx+b_2)
$$
泛化ReLU和Leaky ReLU
1. 线性相关
2. 不会饱和
3. 不会死亡
4. 单个神经元具有的参数翻倍
### 激活函数的选择
1. 优先选ReLu，注意学习率的调整，注意神经元死亡的问题
2. 效果不好尝试leaky_ReLU，Maxout，ELU
3. 可以尝试tanh，但效果不佳
4. 不要使用sigmoid
5. 输出层二分类用sigmoid，多分类用softmax，回归问题用identity
## Data Preprocessing 数据预处理
对数据进行0中心化处理和归一化处理（图像处理领域一般只做0中心化），在训练开始前对所有数据进行处理
1. 0中心化时，减去整个图片的均值（AlexNet）还是每个通道自行减去均值（VGGNet），没有太大区别
2. 不建议进行PCA和白化
## Weight Initialization 权重初始化
1. 初始化为0：参数具有对称性，所有神经元会向同一方向学习
2. 初始化0.01的高斯分布：打破了对称性，小规模网络可以，但更深层的网络，由于权重很小，在神经元的输出就很小，导致反向传播的梯度也很小，最终整个网络的梯度消失
3. 初始化1的高斯分布：参数过大，导致梯度饱和（正负都可能），梯度为0，权重不再更新
4. Xavier：以高斯分布采样为基础，有大量输入时，使用较小的权重；有少量输入时，使用较大的权重。当使用高斯采样初始化和ReLU激活函数时，一半的神经元会死亡，在计算权重初始值时除以2可以解决这一问题
## Batch Normalization 批量归一化
通过批量归一化，使数据呈现单位高斯分布
有利于训练，缓解梯度消失和爆炸，减少初始权重的敏感，允许使用更高的学习率
1. 改善网络梯度流，缓解消失和爆炸
2. 允许更高的学习率，网络拥有更好的健壮性
3. 减少对初始权重的敏感性
4. 可以看做一种正则化
对于批量数据x，计算每个维度的均值和方差，进行归一化
$$
E[x^k]=\frac{1}{N}\sum^{N}_{i=1}x_{k,i}
$$
$$
Var[x^k]=\frac{1}{N}\sum^{N}_{i=1}(x_{k,i}-E[x^k])
$$
$$
\hat{x}^k=\frac{x^k-E[x^k]}{\sqrt{Var[x^k]}}
$$
批量归一化通常在全连接层或卷积层后，非线性层（激活层）之前
对归一化的数据进行缩放和平移操作
$$
y^k=\gamma^k\hat{x}^k+\beta^k
$$
极端情况下可以恢复恒等映射（identity mapping），当
$$
\gamma^k=\sqrt{Var[x^k]}
$$$$
\beta^k=E[x^k]
$$
均值和方差并不是基于当前batch计算的，在训练过程中使用固定的经验所得的值
## Babysitting the Learning Process 监控学习过程
1. 预处理数据
2. 选择网络结构
3. 初始化网络参数：检查损失函数是否符合预期，再加入正则化，检查损失函数是否增大
4. 尝试训练：从小数据集开始，不进行正则化，看损失是否会下降到0
5. 开始正式训练：使用完整的数据集，从一个小的正则化开始，找到合适的学习率（损失下降缓慢，学习率较低，但学习率过大，会导致损失值为NaN，学习率一般为1e-3~1e-5）
- 一开始训练输出可能迅速往好的方向跳跃，是由初始权重对不同的输出是均匀的，所以第一步的学习使用最大的正确值，尽管损失是分散的
## Hyperparameter Optimization 超参数优化
交叉验证，训练集与验证集之间，超参数通常包括网络结构、学习率（衰减时间表、更新形式）、正则化参数（L2/Dropout strength）
1. 首先在样本中选择少量、分散的样本，进行少量epoch的学习，观察参数的变化，快速知道超参数的不同值的影响，得到一个超参数的参数区间
2. 在区间内进行精确搜索，在修改时如果训练成本增加超过三倍吗，则当前的优化方向很有可能是错误的
3. 使用随机探索，比使用贪婪探索（均匀地）更好，贪婪探索由于其均匀性，单个超参数实际只进行了少量的探索（想象井字格的九个点）
### 经验
1. 学习率和损失的关系
![](https://pica.zhimg.com/v2-7a9c8082801526c905efe5d766ffb578_r.jpg)
2. 损失函数出现如下情况，可能是由于一个不太好的初始化，而在某个点碰巧遇到了较好的权重值
![](https://pic1.zhimg.com/v2-a8325f0779871037113a9495e3737686_1440w.jpg)
3. 训练集和验证集之间存在较大的差距，可能是过拟合了，需要提高正则化；没有差距，可能需要增大模型的容量，因为一点过拟合都没有
![](https://pic1.zhimg.com/v2-19d8c0c66ad5d9764c057f5bf74afd16_1440w.jpg)
4. 追踪权重更新率和权重的大小，权重逐步的变化量在0.01~0.001左右一般是正常的